{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import SeparableConv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the initial learning rate, batch size, and number of epochs\n",
    "INIT_LR = 0.01\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 50\n",
    "lr_find = True\n",
    "\n",
    "classes = ['Fire','Non_Fire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "for c in classes:\n",
    "    try:\n",
    "        for img in os.listdir('Image Dataset/'+c):\n",
    "            img = cv2.imread('Image Dataset/'+c+'/'+img)\n",
    "            img = cv2.resize(img,(128,128))\n",
    "            images.append(img)\n",
    "            labels.append([0,1][c=='Fire'])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "images = np.array(images,dtype='float32')/255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just visualizing random images from the dataset created above \n",
    "\n",
    "ind = np.random.randint(0,len(images))\n",
    "cv2.imshow(str(labels[ind]),images[ind])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)\n",
    "labels = np_utils.to_categorical(labels,num_classes=2)\n",
    "\n",
    "classTotals = labels.sum(axis=0)\n",
    "classWeight = classTotals.max() / classTotals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2688., 1315.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classTotals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.       , 2.0441065], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.25, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0331 13:31:29.160035 14916 deprecation_wrapper.py:119] From C:\\Users\\sharma ji\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "separable_conv2d_16 (Separab (None, 128, 128, 32)      155       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_17 (Separab (None, 64, 64, 64)        2400      \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "separable_conv2d_18 (Separab (None, 64, 64, 64)        4736      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               8388736   \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 8,414,461\n",
      "Trainable params: 8,413,629\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# CONV => RELU => POOL\n",
    "model.add(SeparableConv2D(32,(3,3),padding='same',input_shape=(128,128,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# CONV => RELU => CONV => RELU => POOL\n",
    "model.add(SeparableConv2D(64,(3,3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(SeparableConv2D(64,(3,3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# first set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# second set of FC => RELU layers\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# softmax classifier\n",
    "model.add(Dense(len(classes)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "opt = SGD(learning_rate=INIT_LR, momentum=0.9,decay=INIT_LR / NUM_EPOCHS)\n",
    "\n",
    "model.compile(loss=['binary_crossentropy'],\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] finding learning rate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0331 13:35:13.117249 14916 deprecation.py:323] From C:\\Users\\sharma ji\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0331 13:35:13.993090 14916 deprecation_wrapper.py:119] From C:\\Users\\sharma ji\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "47/47 [==============================] - 190s 4s/step - loss: 1.1969 - accuracy: 0.5433\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 192s 4s/step - loss: 1.1686 - accuracy: 0.5586\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 173s 4s/step - loss: 1.1889 - accuracy: 0.5430\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 204s 4s/step - loss: 1.2115 - accuracy: 0.5466\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 194s 4s/step - loss: 1.1721 - accuracy: 0.5553\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 198s 4s/step - loss: 1.1535 - accuracy: 0.5603\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 192s 4s/step - loss: 1.1329 - accuracy: 0.5660\n",
      "Epoch 8/20\n",
      "13/47 [=======>......................] - ETA: 2:49 - loss: 1.1785 - accuracy: 0.5714"
     ]
    }
   ],
   "source": [
    "from pyimagesearch.learningratefinder import LearningRateFinder \n",
    "# if \"lr_find\" ==True:\n",
    "    # initialize the learning rate finder and then train with learning\n",
    "    # rates ranging from 1e-10 to 1e+1\n",
    "print(\"[INFO] finding learning rate...\")\n",
    "lrf = LearningRateFinder(model)\n",
    "# start from 1e-10 till 1e+1\n",
    "lrf.find(\n",
    "    aug.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    1e-10, 1e+1,\n",
    "    stepsPerEpoch=np.ceil((X_train.shape[0] / float(BATCH_SIZE))),\n",
    "    epochs=20,\n",
    "    batchSize=BATCH_SIZE,\n",
    "    classWeight=classWeight)\n",
    "# plot the loss for the various learning rates and save the\n",
    "# resulting plot to disk\n",
    "lrf.plot_loss()\n",
    "plt.savefig(\"output/lrfind_plot.png\")\n",
    "# gracefully exit the script so we can adjust our learning rates\n",
    "# in the config and then train the network for our full set of\n",
    "# epochs\n",
    "print(\"[INFO] learning rate finder complete\")\n",
    "print(\"[INFO] examine plot and adjust learning rates before training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before using it.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-533cf001d947>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclassWeight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     verbose=1)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train_function'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'You must compile your model before using it.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_trainable_weights_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You must compile your model before using it."
     ]
    }
   ],
   "source": [
    "print(\"[INFO] training network...\")\n",
    "\n",
    "H = model.fit_generator(\n",
    "    aug.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    validation_data=(X_test, y_test),\n",
    "    steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    class_weight=classWeight,\n",
    "    verbose=1)\n",
    "\n",
    "print(\"[INFO] serializing network to '{}'...\".format('output/model'))\n",
    "model.save(\"output/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.arange(0, NUM_EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"output/training_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "model = load_model('output/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    random_index = np.random.randint(0,len(images))\n",
    "    org_img = images[random_imdex]\n",
    "    img = org_img.copy()\n",
    "    img = cv2.resize(img,(128,128))\n",
    "    img = img.astype('float32')/255.\n",
    "    pred = model.predict(np.expand_dims(image,axis=0))[0]\n",
    "    result = classes[np.argmax(pred)]\n",
    "    text = 'Warning!! Fire' if result=='Fire' else result\n",
    "    org_img = imutils.resize(org_img, width=500)\n",
    "    cv2.putText(org_img, text, (35, 50), cv2.FONT_HERSHEY_SIMPLEX,1.25, (0, 255, 0), 5)\n",
    "    cv2.imwrite('output/testing/{}.png'.format(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
