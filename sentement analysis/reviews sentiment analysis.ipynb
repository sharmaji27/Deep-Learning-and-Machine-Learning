{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECLARING LEMMATIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(open('electronics/stopwords.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READING POSITIVE AND NEGATIVE REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = BeautifulSoup(open('electronics/positive.review').read())\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('electronics/negative.review').read())\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINING TOKENIZER WHICH CONVERTS STRINGS TO LIST OF WORDS WITH FILTERING OF SMALL WORDS DONE AND LEMMATIZATION ALSO DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(s):\n",
    "    s = s.lower()     # convert the string to lower case\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # make tokens ['dogs', 'the', 'plural', 'for', 'dog']\n",
    "    tokens = [t for t in tokens if len(t)>2] #remove words having length less than 2\n",
    "    tokens = [word_lemmatizer.lemmatize(t) for t in tokens] # lemmatize the words means making different words of same meaning one word like dogs get converted to dog ['dog', 'the', 'plural', 'for', 'dog']\n",
    "    tokens = [t for t in tokens if t not in stop_words] # remove stop words like is,and,this,that etc.\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'the', 'plural', 'for', 'dog']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('dogs is the plural for dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### CREATING WORD_2_INT DICTIONARY WHICH CONATINS ALL THE WORDS AND INDICES AGAINST THEM ACTING AS A UNIQUE KEY FOR THAT WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_int = {}\n",
    "ind = 0\n",
    "positive_tokens = []\n",
    "negative_tokens = []\n",
    "\n",
    "for review in positive_reviews:\n",
    "    tokens = tokenizer(review.text)\n",
    "    positive_tokens.append(tokens)\n",
    "    for t in tokens:\n",
    "        if t not in word_2_int:\n",
    "            word_2_int[t] = ind\n",
    "            ind+=1\n",
    "            \n",
    "for review in negative_reviews:\n",
    "    tokens = tokenizer(review.text)\n",
    "    negative_tokens.append(tokens)\n",
    "    for t in tokens:\n",
    "        if t not in word_2_int:\n",
    "            word_2_int[t] = ind\n",
    "            ind+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting reviews to vectors where each index will depict the count of that word in that review and itslength will be equal to the length of the word_2_int dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_2_vectors(token,label=None):\n",
    "    X = np.zeros(len(word_2_int)+1)\n",
    "    for t in token:\n",
    "        index = word_2_int[t]\n",
    "        X[index]+=1\n",
    "    X = X/X.sum()\n",
    "    X[-1] = label\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacking reviews upon each other in final matrix to build final data(first all positive reviews and then negative reviews) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reviews = len(positive_tokens) + len(negative_tokens)\n",
    "final_matrix = np.zeros((total_reviews , len(word_2_int)+1))\n",
    "row = 0\n",
    "\n",
    "for t in positive_tokens:\n",
    "    final_matrix[row,:] = tokens_2_vectors(t,1)\n",
    "    row+=1\n",
    "    \n",
    "for t in negative_tokens:\n",
    "    final_matrix[row,:] = tokens_2_vectors(t,0)\n",
    "    row+=1\n",
    "\n",
    "np.random.shuffle(final_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_matrix[:,:-1]\n",
    "y = final_matrix[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy is -->  77.07142857142857\n",
      "test accuracy is -->  72.83333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharma ji\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "print('training accuracy is --> ',lr.score(X_train,y_train)*100)\n",
    "print('test accuracy is --> ',lr.score(X_test,y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### just checking the weights of the words and their sentiment according to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and  :  1.346808441780291\n",
      "for  :  1.6150097083838146\n",
      "that  :  -0.6434551112593336\n",
      "are  :  0.6770144147621627\n",
      "the  :  -0.8057102173953095\n",
      "used  :  0.5693761835464781\n",
      "they  :  -0.5699297475678357\n",
      "good  :  1.2099850047922276\n",
      "you  :  0.6617790394504701\n",
      "n't  :  -1.0904400174148612\n",
      "easy  :  0.5789052371772719\n",
      "use  :  0.7688082832962273\n",
      "quality  :  0.7517284661721622\n",
      "best  :  0.5374601511182819\n",
      "very  :  0.7268813035909097\n",
      "with  :  1.000468815174388\n",
      "out  :  -0.7724600813925767\n",
      "price  :  1.2182803269712776\n",
      "great  :  1.997321794142929\n",
      "after  :  -0.9933560584163205\n",
      "worked  :  -0.5098441748584197\n",
      "not  :  -2.520758888992343\n",
      "excellent  :  0.5477471910493635\n",
      "back  :  -0.7754594309862741\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "for word,index in word_2_int.items():\n",
    "    weight = lr.coef_[0][index]\n",
    "    if weight>threshold or weight<-threshold:\n",
    "        print(word,' : ',weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### live predictor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive review\n"
     ]
    }
   ],
   "source": [
    "my_review = 'this product is very good !!'\n",
    "vector = tokens_2_vectors(tokenizer(my_review))\n",
    "vector = vector[:-1]\n",
    "if rfc.predict([vector])[0]==1:\n",
    "    print('positive review')\n",
    "else:\n",
    "    print('negative review')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I TRIED USING RANDOM FOREST BUT IT WAS COMPLETELY OVERFITTING GIVING 100% TRAINING ACCURACY AND VERY LOW TEST ACCURACY ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I ALSO TRIED SVM BUT THAT ALSO PERFORMED REALLY BAD !!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
