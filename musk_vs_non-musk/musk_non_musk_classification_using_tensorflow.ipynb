{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required packages\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading csv file\n",
    "data = pd.read_csv('musk_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>conformation_name</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>...</th>\n",
       "      <th>f158</th>\n",
       "      <th>f159</th>\n",
       "      <th>f160</th>\n",
       "      <th>f161</th>\n",
       "      <th>f162</th>\n",
       "      <th>f163</th>\n",
       "      <th>f164</th>\n",
       "      <th>f165</th>\n",
       "      <th>f166</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MUSK-211</td>\n",
       "      <td>211_1+1</td>\n",
       "      <td>46</td>\n",
       "      <td>-108</td>\n",
       "      <td>-60</td>\n",
       "      <td>-69</td>\n",
       "      <td>-117</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>-308</td>\n",
       "      <td>52</td>\n",
       "      <td>-7</td>\n",
       "      <td>39</td>\n",
       "      <td>126</td>\n",
       "      <td>156</td>\n",
       "      <td>-50</td>\n",
       "      <td>-112</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>MUSK-211</td>\n",
       "      <td>211_1+10</td>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-6</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>-59</td>\n",
       "      <td>-2</td>\n",
       "      <td>52</td>\n",
       "      <td>103</td>\n",
       "      <td>136</td>\n",
       "      <td>169</td>\n",
       "      <td>-61</td>\n",
       "      <td>-136</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>MUSK-211</td>\n",
       "      <td>211_1+11</td>\n",
       "      <td>46</td>\n",
       "      <td>-194</td>\n",
       "      <td>-145</td>\n",
       "      <td>28</td>\n",
       "      <td>-117</td>\n",
       "      <td>73</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>-134</td>\n",
       "      <td>-154</td>\n",
       "      <td>57</td>\n",
       "      <td>143</td>\n",
       "      <td>142</td>\n",
       "      <td>165</td>\n",
       "      <td>-67</td>\n",
       "      <td>-145</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>MUSK-211</td>\n",
       "      <td>211_1+12</td>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>136</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>MUSK-211</td>\n",
       "      <td>211_1+13</td>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>137</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID molecule_name conformation_name  f1   f2   f3  f4   f5  f6  f7  ...  \\\n",
       "0   1      MUSK-211           211_1+1  46 -108  -60 -69 -117  49  38  ...   \n",
       "1   2      MUSK-211          211_1+10  41 -188 -145  22 -117  -6  57  ...   \n",
       "2   3      MUSK-211          211_1+11  46 -194 -145  28 -117  73  57  ...   \n",
       "3   4      MUSK-211          211_1+12  41 -188 -145  22 -117  -7  57  ...   \n",
       "4   5      MUSK-211          211_1+13  41 -188 -145  22 -117  -7  57  ...   \n",
       "\n",
       "   f158  f159  f160  f161  f162  f163  f164  f165  f166  class  \n",
       "0  -308    52    -7    39   126   156   -50  -112    96      1  \n",
       "1   -59    -2    52   103   136   169   -61  -136    79      1  \n",
       "2  -134  -154    57   143   142   165   -67  -145    39      1  \n",
       "3   -60    -4    52   104   136   168   -60  -135    80      1  \n",
       "4   -60    -4    52   104   137   168   -60  -135    80      1  \n",
       "\n",
       "[5 rows x 170 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking head of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f158</th>\n",
       "      <th>f159</th>\n",
       "      <th>f160</th>\n",
       "      <th>f161</th>\n",
       "      <th>f162</th>\n",
       "      <th>f163</th>\n",
       "      <th>f164</th>\n",
       "      <th>f165</th>\n",
       "      <th>f166</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>6598.00000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>3299.50000</td>\n",
       "      <td>58.945135</td>\n",
       "      <td>-119.128524</td>\n",
       "      <td>-73.146560</td>\n",
       "      <td>-0.628372</td>\n",
       "      <td>-103.533495</td>\n",
       "      <td>18.359806</td>\n",
       "      <td>-14.108821</td>\n",
       "      <td>-1.858290</td>\n",
       "      <td>-86.003031</td>\n",
       "      <td>...</td>\n",
       "      <td>-184.798272</td>\n",
       "      <td>-75.795696</td>\n",
       "      <td>-26.073204</td>\n",
       "      <td>64.616702</td>\n",
       "      <td>112.037739</td>\n",
       "      <td>201.760230</td>\n",
       "      <td>-47.488330</td>\n",
       "      <td>-150.259927</td>\n",
       "      <td>41.770233</td>\n",
       "      <td>0.154138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>1904.82287</td>\n",
       "      <td>53.249007</td>\n",
       "      <td>90.813375</td>\n",
       "      <td>67.956235</td>\n",
       "      <td>80.444617</td>\n",
       "      <td>64.387559</td>\n",
       "      <td>80.593655</td>\n",
       "      <td>115.315673</td>\n",
       "      <td>90.372537</td>\n",
       "      <td>108.326676</td>\n",
       "      <td>...</td>\n",
       "      <td>107.819514</td>\n",
       "      <td>127.861271</td>\n",
       "      <td>69.727964</td>\n",
       "      <td>100.861935</td>\n",
       "      <td>72.835040</td>\n",
       "      <td>59.526751</td>\n",
       "      <td>55.069365</td>\n",
       "      <td>76.019023</td>\n",
       "      <td>94.116085</td>\n",
       "      <td>0.361108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-31.000000</td>\n",
       "      <td>-199.000000</td>\n",
       "      <td>-167.000000</td>\n",
       "      <td>-114.000000</td>\n",
       "      <td>-118.000000</td>\n",
       "      <td>-183.000000</td>\n",
       "      <td>-171.000000</td>\n",
       "      <td>-225.000000</td>\n",
       "      <td>-245.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-328.000000</td>\n",
       "      <td>-219.000000</td>\n",
       "      <td>-136.000000</td>\n",
       "      <td>-120.000000</td>\n",
       "      <td>-69.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>-289.000000</td>\n",
       "      <td>-428.000000</td>\n",
       "      <td>-471.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>1650.25000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>-193.000000</td>\n",
       "      <td>-137.000000</td>\n",
       "      <td>-70.000000</td>\n",
       "      <td>-117.000000</td>\n",
       "      <td>-28.000000</td>\n",
       "      <td>-159.000000</td>\n",
       "      <td>-85.000000</td>\n",
       "      <td>-217.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-272.000000</td>\n",
       "      <td>-205.000000</td>\n",
       "      <td>-70.000000</td>\n",
       "      <td>-18.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>-68.000000</td>\n",
       "      <td>-179.000000</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>3299.50000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>-149.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-25.000000</td>\n",
       "      <td>-117.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-234.000000</td>\n",
       "      <td>-131.000000</td>\n",
       "      <td>-21.000000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>-150.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>4948.75000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-19.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>-116.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>-21.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>-45.000000</td>\n",
       "      <td>-120.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>6598.00000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>355.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID           f1           f2           f3           f4  \\\n",
       "count  6598.00000  6598.000000  6598.000000  6598.000000  6598.000000   \n",
       "mean   3299.50000    58.945135  -119.128524   -73.146560    -0.628372   \n",
       "std    1904.82287    53.249007    90.813375    67.956235    80.444617   \n",
       "min       1.00000   -31.000000  -199.000000  -167.000000  -114.000000   \n",
       "25%    1650.25000    37.000000  -193.000000  -137.000000   -70.000000   \n",
       "50%    3299.50000    44.000000  -149.000000   -99.000000   -25.000000   \n",
       "75%    4948.75000    53.000000   -95.000000   -19.000000    42.000000   \n",
       "max    6598.00000   292.000000    95.000000    81.000000   161.000000   \n",
       "\n",
       "                f5           f6           f7           f8           f9  ...  \\\n",
       "count  6598.000000  6598.000000  6598.000000  6598.000000  6598.000000  ...   \n",
       "mean   -103.533495    18.359806   -14.108821    -1.858290   -86.003031  ...   \n",
       "std      64.387559    80.593655   115.315673    90.372537   108.326676  ...   \n",
       "min    -118.000000  -183.000000  -171.000000  -225.000000  -245.000000  ...   \n",
       "25%    -117.000000   -28.000000  -159.000000   -85.000000  -217.000000  ...   \n",
       "50%    -117.000000    33.000000    27.000000    19.000000   -40.000000  ...   \n",
       "75%    -116.000000    74.000000    57.000000    61.000000   -21.000000  ...   \n",
       "max     325.000000   200.000000   220.000000   320.000000   147.000000  ...   \n",
       "\n",
       "              f158         f159         f160         f161         f162  \\\n",
       "count  6598.000000  6598.000000  6598.000000  6598.000000  6598.000000   \n",
       "mean   -184.798272   -75.795696   -26.073204    64.616702   112.037739   \n",
       "std     107.819514   127.861271    69.727964   100.861935    72.835040   \n",
       "min    -328.000000  -219.000000  -136.000000  -120.000000   -69.000000   \n",
       "25%    -272.000000  -205.000000   -70.000000   -18.000000    71.000000   \n",
       "50%    -234.000000  -131.000000   -21.000000    61.500000   107.000000   \n",
       "75%     -80.000000    52.000000     9.000000   149.000000   129.000000   \n",
       "max      94.000000   179.000000   192.000000   411.000000   355.000000   \n",
       "\n",
       "              f163         f164         f165         f166        class  \n",
       "count  6598.000000  6598.000000  6598.000000  6598.000000  6598.000000  \n",
       "mean    201.760230   -47.488330  -150.259927    41.770233     0.154138  \n",
       "std      59.526751    55.069365    76.019023    94.116085     0.361108  \n",
       "min      73.000000  -289.000000  -428.000000  -471.000000     0.000000  \n",
       "25%     166.000000   -68.000000  -179.000000    -9.000000     0.000000  \n",
       "50%     191.000000   -60.000000  -150.000000    27.000000     0.000000  \n",
       "75%     215.000000   -45.000000  -120.000000   119.000000     0.000000  \n",
       "max     625.000000   295.000000   168.000000   367.000000     1.000000  \n",
       "\n",
       "[8 rows x 168 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just some stats about our data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6598 entries, 0 to 6597\n",
      "Columns: 170 entries, ID to class\n",
      "dtypes: int64(168), object(2)\n",
      "memory usage: 8.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_dropping = ['ID','molecule_name','conformation_name','class']\n",
    "\n",
    "# shuffling the data\n",
    "data = shuffle(data)\n",
    "\n",
    "#dropping all waste columns \n",
    "X = data.drop(columns_for_dropping,axis=1)\n",
    "y = data['class']\n",
    "\n",
    "# declaring scaler and scaling the data\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X))\n",
    "\n",
    "#splitting the data as said in 80:20 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,shuffle=True)\n",
    "y_train = np.reshape(np.array(y_train),[-1,1]) \n",
    "y_test  = np.reshape(np.array(y_test),[-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1215 20:36:16.773367 14448 deprecation.py:506] From C:\\Users\\sharma ji\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1215 20:36:16.810801 14448 deprecation.py:323] From <ipython-input-7-e4b4de082395>:31: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "W1215 20:36:16.892004 14448 deprecation.py:506] From C:\\Users\\sharma ji\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1215 20:36:16.938902 14448 deprecation.py:323] From <ipython-input-7-e4b4de082395>:43: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.math.argmax` instead\n"
     ]
    }
   ],
   "source": [
    "#making placeholders\n",
    "X = tf.placeholder(tf.float32,shape=[None,166])\n",
    "targets = tf.placeholder(tf.int32,shape=[None,1])\n",
    "\n",
    "#declaring no of nodes\n",
    "nodes_in_hidden_layer_1 = 200\n",
    "nodes_in_hidden_layer_2 = 100\n",
    "nodes_in_hidden_layer_3 = 2\n",
    "\n",
    "# making the neural network\n",
    "\n",
    "# hiddden layer 1\n",
    "weights_1 = tf.get_variable('weights_1',shape=[X.shape[1],nodes_in_hidden_layer_1]) \n",
    "bias_1 = tf.Variable(tf.zeros(nodes_in_hidden_layer_1))\n",
    "# bias_1  = tf.get_variable('bias_1',nodes_in_hidden_layer_1)\n",
    "output_1  = tf.nn.tanh(tf.matmul(X,weights_1)+bias_1)\n",
    "\n",
    "# hiddden layer 2\n",
    "weights_2 = tf.get_variable('weights_2',shape=[output_1.shape[1],nodes_in_hidden_layer_2]) \n",
    "bias_2 = tf.Variable(tf.zeros(nodes_in_hidden_layer_2))\n",
    "# bias_2  = tf.get_variable('bias_2',nodes_in_hidden_layer_2)\n",
    "output_2  = tf.nn.tanh(tf.matmul(output_1,weights_2)+bias_2)\n",
    "\n",
    "# hiddden layer 3\n",
    "weights_3 = tf.get_variable('weights_3',shape=[output_2.shape[1],nodes_in_hidden_layer_3]) \n",
    "bias_3 = tf.Variable(tf.zeros(nodes_in_hidden_layer_3))\n",
    "# bias_3  = tf.get_variable('bias_3',nodes_in_hidden_layer_3)\n",
    "output  = tf.nn.sigmoid(tf.matmul(output_2,weights_3)+bias_3)\n",
    "\n",
    "# defining the loss and optimizer\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=targets,logits=output)\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(mean_loss)\n",
    "\n",
    "#maximum accuracies for 500 epochs\n",
    "#adam             --> 83.4%(testing)  84.8%(training) \n",
    "#rms prop         --> 84.7%(testing)  83%(training)\n",
    "#gradient_descent --> 49.84%(testing) 50.38%(training)\n",
    "#adadelta         --> 48%(testing)    48%(training)\n",
    "#adagrad          --> 51%(testing)    51%(training)\n",
    "\n",
    "# getting the accuracy\n",
    "accuracy = tf.equal(tf.arg_max(output,1,output_type=tf.int32),targets)\n",
    "accuracy = tf.reduce_mean(tf.cast(accuracy,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declatring th tensorflow session\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring batching function to make batches\n",
    "\n",
    "def batching(X,y,batch_size,no_of_batches):\n",
    "    no_of_batches = X.shape[0]//batch_size\n",
    "    \n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    \n",
    "    for i in range(no_of_batches):\n",
    "        if i==no_of_batches-1:\n",
    "            X_batch.append(X[batch_size*i:])\n",
    "            y_batch.append(y[batch_size*i:])\n",
    "        else:\n",
    "            X_batch.append(X[batch_size*i:batch_size*(i+1)])\n",
    "            y_batch.append(y[batch_size*i:batch_size*(i+1)])\n",
    "    \n",
    "    return X_batch,y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for epoch  0  -->  0.21446\n",
      "training_accuracy for epoch  0  -->  0.580931\n",
      "testing_accuracy for epoch  0  -->  0.5788625\n",
      "\n",
      "\n",
      "loss for epoch  1  -->  0.2133\n",
      "training_accuracy for epoch  1  -->  0.6185764\n",
      "testing_accuracy for epoch  1  -->  0.6175103\n",
      "\n",
      "\n",
      "loss for epoch  2  -->  0.21329\n",
      "training_accuracy for epoch  2  -->  0.5667648\n",
      "testing_accuracy for epoch  2  -->  0.56528354\n",
      "\n",
      "\n",
      "loss for epoch  3  -->  0.21329\n",
      "training_accuracy for epoch  3  -->  0.55063105\n",
      "testing_accuracy for epoch  3  -->  0.54648185\n",
      "\n",
      "\n",
      "loss for epoch  4  -->  0.21329\n",
      "training_accuracy for epoch  4  -->  0.64782697\n",
      "testing_accuracy for epoch  4  -->  0.6446683\n",
      "\n",
      "\n",
      "loss for epoch  5  -->  0.21329\n",
      "training_accuracy for epoch  5  -->  0.68179965\n",
      "testing_accuracy for epoch  5  -->  0.6901056\n",
      "\n",
      "\n",
      "loss for epoch  6  -->  0.21329\n",
      "training_accuracy for epoch  6  -->  0.7139359\n",
      "testing_accuracy for epoch  6  -->  0.71674126\n",
      "\n",
      "\n",
      "loss for epoch  7  -->  0.21329\n",
      "training_accuracy for epoch  7  -->  0.72233075\n",
      "testing_accuracy for epoch  7  -->  0.72561985\n",
      "\n",
      "\n",
      "loss for epoch  8  -->  0.21329\n",
      "training_accuracy for epoch  8  -->  0.71695274\n",
      "testing_accuracy for epoch  8  -->  0.69898415\n",
      "\n",
      "\n",
      "loss for epoch  9  -->  0.21329\n",
      "training_accuracy for epoch  9  -->  0.75787735\n",
      "testing_accuracy for epoch  9  -->  0.7438992\n",
      "\n",
      "\n",
      "loss for epoch  10  -->  0.21329\n",
      "training_accuracy for epoch  10  -->  0.75971377\n",
      "testing_accuracy for epoch  10  -->  0.7501664\n",
      "\n",
      "\n",
      "loss for epoch  11  -->  0.21329\n",
      "training_accuracy for epoch  11  -->  0.7792579\n",
      "testing_accuracy for epoch  11  -->  0.7783689\n",
      "\n",
      "\n",
      "loss for epoch  12  -->  0.21329\n",
      "training_accuracy for epoch  12  -->  0.7633864\n",
      "testing_accuracy for epoch  12  -->  0.7559114\n",
      "\n",
      "\n",
      "loss for epoch  13  -->  0.21329\n",
      "training_accuracy for epoch  13  -->  0.75682807\n",
      "testing_accuracy for epoch  13  -->  0.75068873\n",
      "\n",
      "\n",
      "loss for epoch  14  -->  0.21329\n",
      "training_accuracy for epoch  14  -->  0.7750605\n",
      "testing_accuracy for epoch  14  -->  0.7642677\n",
      "\n",
      "\n",
      "loss for epoch  15  -->  0.21329\n",
      "training_accuracy for epoch  15  -->  0.7848981\n",
      "testing_accuracy for epoch  15  -->  0.77941346\n",
      "\n",
      "\n",
      "loss for epoch  16  -->  0.21329\n",
      "training_accuracy for epoch  16  -->  0.74056315\n",
      "testing_accuracy for epoch  16  -->  0.74181014\n",
      "\n",
      "\n",
      "loss for epoch  17  -->  0.21329\n",
      "training_accuracy for epoch  17  -->  0.7379398\n",
      "testing_accuracy for epoch  17  -->  0.7329316\n",
      "\n",
      "\n",
      "loss for epoch  18  -->  0.21329\n",
      "training_accuracy for epoch  18  -->  0.78056955\n",
      "testing_accuracy for epoch  18  -->  0.77471304\n",
      "\n",
      "\n",
      "loss for epoch  19  -->  0.21329\n",
      "training_accuracy for epoch  19  -->  0.7666657\n",
      "testing_accuracy for epoch  19  -->  0.7606118\n",
      "\n",
      "\n",
      "loss for epoch  20  -->  0.21329\n",
      "training_accuracy for epoch  20  -->  0.7393826\n",
      "testing_accuracy for epoch  20  -->  0.73658746\n",
      "\n",
      "\n",
      "loss for epoch  21  -->  0.21329\n",
      "training_accuracy for epoch  21  -->  0.7608943\n",
      "testing_accuracy for epoch  21  -->  0.75800043\n",
      "\n",
      "\n",
      "loss for epoch  22  -->  0.21329\n",
      "training_accuracy for epoch  22  -->  0.7728306\n",
      "testing_accuracy for epoch  22  -->  0.7710572\n",
      "\n",
      "\n",
      "loss for epoch  23  -->  0.21329\n",
      "training_accuracy for epoch  23  -->  0.7695514\n",
      "testing_accuracy for epoch  23  -->  0.76165634\n",
      "\n",
      "\n",
      "loss for epoch  24  -->  0.21329\n",
      "training_accuracy for epoch  24  -->  0.785554\n",
      "testing_accuracy for epoch  24  -->  0.7799357\n",
      "\n",
      "\n",
      "loss for epoch  25  -->  0.21329\n",
      "training_accuracy for epoch  25  -->  0.75446707\n",
      "testing_accuracy for epoch  25  -->  0.74964416\n",
      "\n",
      "\n",
      "loss for epoch  26  -->  0.21329\n",
      "training_accuracy for epoch  26  -->  0.75879556\n",
      "testing_accuracy for epoch  26  -->  0.7533001\n",
      "\n",
      "\n",
      "loss for epoch  27  -->  0.21329\n",
      "training_accuracy for epoch  27  -->  0.7657475\n",
      "testing_accuracy for epoch  27  -->  0.7637454\n",
      "\n",
      "\n",
      "loss for epoch  28  -->  0.21329\n",
      "training_accuracy for epoch  28  -->  0.75879556\n",
      "testing_accuracy for epoch  28  -->  0.75956726\n",
      "\n",
      "\n",
      "loss for epoch  29  -->  0.21329\n",
      "training_accuracy for epoch  29  -->  0.77978253\n",
      "testing_accuracy for epoch  29  -->  0.7768021\n",
      "\n",
      "\n",
      "loss for epoch  30  -->  0.21329\n",
      "training_accuracy for epoch  30  -->  0.78371763\n",
      "testing_accuracy for epoch  30  -->  0.77471304\n",
      "\n",
      "\n",
      "loss for epoch  31  -->  0.21329\n",
      "training_accuracy for epoch  31  -->  0.780176\n",
      "testing_accuracy for epoch  31  -->  0.7721017\n",
      "\n",
      "\n",
      "loss for epoch  32  -->  0.21329\n",
      "training_accuracy for epoch  32  -->  0.77624106\n",
      "testing_accuracy for epoch  32  -->  0.76949036\n",
      "\n",
      "\n",
      "loss for epoch  33  -->  0.21329\n",
      "training_accuracy for epoch  33  -->  0.7877839\n",
      "testing_accuracy for epoch  33  -->  0.7783689\n",
      "\n",
      "\n",
      "loss for epoch  34  -->  0.21329\n",
      "training_accuracy for epoch  34  -->  0.7800449\n",
      "testing_accuracy for epoch  34  -->  0.78150254\n",
      "\n",
      "\n",
      "loss for epoch  35  -->  0.21328\n",
      "training_accuracy for epoch  35  -->  0.77309304\n",
      "testing_accuracy for epoch  35  -->  0.7700126\n",
      "\n",
      "\n",
      "loss for epoch  36  -->  0.21329\n",
      "training_accuracy for epoch  36  -->  0.7850293\n",
      "testing_accuracy for epoch  36  -->  0.7820248\n",
      "\n",
      "\n",
      "loss for epoch  37  -->  0.21329\n",
      "training_accuracy for epoch  37  -->  0.80063844\n",
      "testing_accuracy for epoch  37  -->  0.79769284\n",
      "\n",
      "\n",
      "loss for epoch  38  -->  0.21329\n",
      "training_accuracy for epoch  38  -->  0.7859475\n",
      "testing_accuracy for epoch  38  -->  0.7830693\n",
      "\n",
      "\n",
      "loss for epoch  39  -->  0.21328\n",
      "training_accuracy for epoch  39  -->  0.79972017\n",
      "testing_accuracy for epoch  39  -->  0.7971706\n",
      "\n",
      "\n",
      "loss for epoch  40  -->  0.21328\n",
      "training_accuracy for epoch  40  -->  0.7944734\n",
      "testing_accuracy for epoch  40  -->  0.7851584\n",
      "\n",
      "\n",
      "loss for epoch  41  -->  0.21328\n",
      "training_accuracy for epoch  41  -->  0.78909546\n",
      "testing_accuracy for epoch  41  -->  0.7799357\n",
      "\n",
      "\n",
      "loss for epoch  42  -->  0.21329\n",
      "training_accuracy for epoch  42  -->  0.8032618\n",
      "testing_accuracy for epoch  42  -->  0.79090333\n",
      "\n",
      "\n",
      "loss for epoch  43  -->  0.21328\n",
      "training_accuracy for epoch  43  -->  0.8113943\n",
      "testing_accuracy for epoch  43  -->  0.8060491\n",
      "\n",
      "\n",
      "loss for epoch  44  -->  0.21329\n",
      "training_accuracy for epoch  44  -->  0.81401753\n",
      "testing_accuracy for epoch  44  -->  0.80239326\n",
      "\n",
      "\n",
      "loss for epoch  45  -->  0.21328\n",
      "training_accuracy for epoch  45  -->  0.813624\n",
      "testing_accuracy for epoch  45  -->  0.8065714\n",
      "\n",
      "\n",
      "loss for epoch  46  -->  0.21328\n",
      "training_accuracy for epoch  46  -->  0.81467336\n",
      "testing_accuracy for epoch  46  -->  0.8050046\n",
      "\n",
      "\n",
      "loss for epoch  47  -->  0.21328\n",
      "training_accuracy for epoch  47  -->  0.8165097\n",
      "testing_accuracy for epoch  47  -->  0.80448234\n",
      "\n",
      "\n",
      "loss for epoch  48  -->  0.21328\n",
      "training_accuracy for epoch  48  -->  0.816641\n",
      "testing_accuracy for epoch  48  -->  0.809705\n",
      "\n",
      "\n",
      "loss for epoch  49  -->  0.21328\n",
      "training_accuracy for epoch  49  -->  0.8199201\n",
      "testing_accuracy for epoch  49  -->  0.8060491\n",
      "\n",
      "\n",
      "loss for epoch  50  -->  0.21328\n",
      "training_accuracy for epoch  50  -->  0.8167721\n",
      "testing_accuracy for epoch  50  -->  0.80866045\n",
      "\n",
      "\n",
      "loss for epoch  51  -->  0.21328\n",
      "training_accuracy for epoch  51  -->  0.8165097\n",
      "testing_accuracy for epoch  51  -->  0.8060491\n",
      "\n",
      "\n",
      "loss for epoch  52  -->  0.21328\n",
      "training_accuracy for epoch  52  -->  0.81900203\n",
      "testing_accuracy for epoch  52  -->  0.80918276\n",
      "\n",
      "\n",
      "loss for epoch  53  -->  0.21328\n",
      "training_accuracy for epoch  53  -->  0.81900185\n",
      "testing_accuracy for epoch  53  -->  0.81231636\n",
      "\n",
      "\n",
      "loss for epoch  54  -->  0.21328\n",
      "training_accuracy for epoch  54  -->  0.817428\n",
      "testing_accuracy for epoch  54  -->  0.8081382\n",
      "\n",
      "\n",
      "loss for epoch  55  -->  0.21328\n",
      "training_accuracy for epoch  55  -->  0.8207072\n",
      "testing_accuracy for epoch  55  -->  0.809705\n",
      "\n",
      "\n",
      "loss for epoch  56  -->  0.21328\n",
      "training_accuracy for epoch  56  -->  0.8192643\n",
      "testing_accuracy for epoch  56  -->  0.80918276\n",
      "\n",
      "\n",
      "loss for epoch  57  -->  0.21328\n",
      "training_accuracy for epoch  57  -->  0.8188708\n",
      "testing_accuracy for epoch  57  -->  0.81336087\n",
      "\n",
      "\n",
      "loss for epoch  58  -->  0.21328\n",
      "training_accuracy for epoch  58  -->  0.81926435\n",
      "testing_accuracy for epoch  58  -->  0.81074953\n",
      "\n",
      "\n",
      "loss for epoch  59  -->  0.21328\n",
      "training_accuracy for epoch  59  -->  0.82031363\n",
      "testing_accuracy for epoch  59  -->  0.8081382\n",
      "\n",
      "\n",
      "loss for epoch  60  -->  0.21328\n",
      "training_accuracy for epoch  60  -->  0.82018256\n",
      "testing_accuracy for epoch  60  -->  0.81231636\n",
      "\n",
      "\n",
      "loss for epoch  61  -->  0.21328\n",
      "training_accuracy for epoch  61  -->  0.8207071\n",
      "testing_accuracy for epoch  61  -->  0.8102273\n",
      "\n",
      "\n",
      "loss for epoch  62  -->  0.21328\n",
      "training_accuracy for epoch  62  -->  0.8217565\n",
      "testing_accuracy for epoch  62  -->  0.81074953\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for epoch  63  -->  0.21328\n",
      "training_accuracy for epoch  63  -->  0.8217565\n",
      "testing_accuracy for epoch  63  -->  0.8112718\n",
      "\n",
      "\n",
      "loss for epoch  64  -->  0.21328\n",
      "training_accuracy for epoch  64  -->  0.8196578\n",
      "testing_accuracy for epoch  64  -->  0.8138831\n",
      "\n",
      "\n",
      "loss for epoch  65  -->  0.21328\n",
      "training_accuracy for epoch  65  -->  0.8226747\n",
      "testing_accuracy for epoch  65  -->  0.8112718\n",
      "\n",
      "\n",
      "loss for epoch  66  -->  0.21328\n",
      "training_accuracy for epoch  66  -->  0.82136303\n",
      "testing_accuracy for epoch  66  -->  0.81074953\n",
      "\n",
      "\n",
      "loss for epoch  67  -->  0.21328\n",
      "training_accuracy for epoch  67  -->  0.8217565\n",
      "testing_accuracy for epoch  67  -->  0.81231636\n",
      "\n",
      "\n",
      "loss for epoch  68  -->  0.21328\n",
      "training_accuracy for epoch  68  -->  0.82123184\n",
      "testing_accuracy for epoch  68  -->  0.8102273\n",
      "\n",
      "\n",
      "loss for epoch  69  -->  0.21328\n",
      "training_accuracy for epoch  69  -->  0.82215005\n",
      "testing_accuracy for epoch  69  -->  0.8149277\n",
      "\n",
      "\n",
      "loss for epoch  70  -->  0.21328\n",
      "training_accuracy for epoch  70  -->  0.8222812\n",
      "testing_accuracy for epoch  70  -->  0.8138831\n",
      "\n",
      "\n",
      "loss for epoch  71  -->  0.21328\n",
      "training_accuracy for epoch  71  -->  0.82096946\n",
      "testing_accuracy for epoch  71  -->  0.81440544\n",
      "\n",
      "\n",
      "loss for epoch  72  -->  0.21328\n",
      "training_accuracy for epoch  72  -->  0.8216254\n",
      "testing_accuracy for epoch  72  -->  0.81231636\n",
      "\n",
      "\n",
      "loss for epoch  73  -->  0.21328\n",
      "training_accuracy for epoch  73  -->  0.8237241\n",
      "testing_accuracy for epoch  73  -->  0.8149277\n",
      "\n",
      "\n",
      "loss for epoch  74  -->  0.21328\n",
      "training_accuracy for epoch  74  -->  0.8222812\n",
      "testing_accuracy for epoch  74  -->  0.81544995\n",
      "\n",
      "\n",
      "loss for epoch  75  -->  0.21328\n",
      "training_accuracy for epoch  75  -->  0.8256915\n",
      "testing_accuracy for epoch  75  -->  0.81440544\n",
      "\n",
      "\n",
      "loss for epoch  76  -->  0.21328\n",
      "training_accuracy for epoch  76  -->  0.82241225\n",
      "testing_accuracy for epoch  76  -->  0.81858355\n",
      "\n",
      "\n",
      "loss for epoch  77  -->  0.21328\n",
      "training_accuracy for epoch  77  -->  0.8250357\n",
      "testing_accuracy for epoch  77  -->  0.81440544\n",
      "\n",
      "\n",
      "loss for epoch  78  -->  0.21328\n",
      "training_accuracy for epoch  78  -->  0.82293695\n",
      "testing_accuracy for epoch  78  -->  0.8149277\n",
      "\n",
      "\n",
      "loss for epoch  79  -->  0.21328\n",
      "training_accuracy for epoch  79  -->  0.82529813\n",
      "testing_accuracy for epoch  79  -->  0.8149277\n",
      "\n",
      "\n",
      "loss for epoch  80  -->  0.21328\n",
      "training_accuracy for epoch  80  -->  0.82241225\n",
      "testing_accuracy for epoch  80  -->  0.8159722\n",
      "\n",
      "\n",
      "loss for epoch  81  -->  0.21328\n",
      "training_accuracy for epoch  81  -->  0.82359284\n",
      "testing_accuracy for epoch  81  -->  0.81440544\n",
      "\n",
      "\n",
      "loss for epoch  82  -->  0.21328\n",
      "training_accuracy for epoch  82  -->  0.82215\n",
      "testing_accuracy for epoch  82  -->  0.8159722\n",
      "\n",
      "\n",
      "loss for epoch  83  -->  0.21328\n",
      "training_accuracy for epoch  83  -->  0.8242488\n",
      "testing_accuracy for epoch  83  -->  0.81649446\n",
      "\n",
      "\n",
      "loss for epoch  84  -->  0.21328\n",
      "training_accuracy for epoch  84  -->  0.8239863\n",
      "testing_accuracy for epoch  84  -->  0.81231636\n",
      "\n",
      "\n",
      "loss for epoch  85  -->  0.21328\n",
      "training_accuracy for epoch  85  -->  0.8218876\n",
      "testing_accuracy for epoch  85  -->  0.81336087\n",
      "\n",
      "\n",
      "loss for epoch  86  -->  0.21328\n",
      "training_accuracy for epoch  86  -->  0.82149416\n",
      "testing_accuracy for epoch  86  -->  0.81753904\n",
      "\n",
      "\n",
      "loss for epoch  87  -->  0.21328\n",
      "training_accuracy for epoch  87  -->  0.8224123\n",
      "testing_accuracy for epoch  87  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  88  -->  0.21328\n",
      "training_accuracy for epoch  88  -->  0.82437986\n",
      "testing_accuracy for epoch  88  -->  0.81649446\n",
      "\n",
      "\n",
      "loss for epoch  89  -->  0.21328\n",
      "training_accuracy for epoch  89  -->  0.82398635\n",
      "testing_accuracy for epoch  89  -->  0.809705\n",
      "\n",
      "\n",
      "loss for epoch  90  -->  0.21328\n",
      "training_accuracy for epoch  90  -->  0.8235928\n",
      "testing_accuracy for epoch  90  -->  0.8149277\n",
      "\n",
      "\n",
      "loss for epoch  91  -->  0.21328\n",
      "training_accuracy for epoch  91  -->  0.8254292\n",
      "testing_accuracy for epoch  91  -->  0.81544995\n",
      "\n",
      "\n",
      "loss for epoch  92  -->  0.21328\n",
      "training_accuracy for epoch  92  -->  0.8220189\n",
      "testing_accuracy for epoch  92  -->  0.8191058\n",
      "\n",
      "\n",
      "loss for epoch  93  -->  0.21328\n",
      "training_accuracy for epoch  93  -->  0.8225435\n",
      "testing_accuracy for epoch  93  -->  0.8170168\n",
      "\n",
      "\n",
      "loss for epoch  94  -->  0.21328\n",
      "training_accuracy for epoch  94  -->  0.82451105\n",
      "testing_accuracy for epoch  94  -->  0.8159722\n",
      "\n",
      "\n",
      "loss for epoch  95  -->  0.21328\n",
      "training_accuracy for epoch  95  -->  0.8220188\n",
      "testing_accuracy for epoch  95  -->  0.81544995\n",
      "\n",
      "\n",
      "loss for epoch  96  -->  0.21328\n",
      "training_accuracy for epoch  96  -->  0.8258227\n",
      "testing_accuracy for epoch  96  -->  0.81649446\n",
      "\n",
      "\n",
      "loss for epoch  97  -->  0.21328\n",
      "training_accuracy for epoch  97  -->  0.82385516\n",
      "testing_accuracy for epoch  97  -->  0.81074953\n",
      "\n",
      "\n",
      "loss for epoch  98  -->  0.21328\n",
      "training_accuracy for epoch  98  -->  0.82319933\n",
      "testing_accuracy for epoch  98  -->  0.81544995\n",
      "\n",
      "\n",
      "loss for epoch  99  -->  0.21328\n",
      "training_accuracy for epoch  99  -->  0.8251668\n",
      "testing_accuracy for epoch  99  -->  0.8191058\n",
      "\n",
      "\n",
      "loss for epoch  100  -->  0.21328\n",
      "training_accuracy for epoch  100  -->  0.82529795\n",
      "testing_accuracy for epoch  100  -->  0.8159722\n",
      "\n",
      "\n",
      "loss for epoch  101  -->  0.21328\n",
      "training_accuracy for epoch  101  -->  0.8258227\n",
      "testing_accuracy for epoch  101  -->  0.8180613\n",
      "\n",
      "\n",
      "loss for epoch  102  -->  0.21328\n",
      "training_accuracy for epoch  102  -->  0.82411754\n",
      "testing_accuracy for epoch  102  -->  0.81649446\n",
      "\n",
      "\n",
      "loss for epoch  103  -->  0.21328\n",
      "training_accuracy for epoch  103  -->  0.82451105\n",
      "testing_accuracy for epoch  103  -->  0.8159722\n",
      "\n",
      "\n",
      "loss for epoch  104  -->  0.21328\n",
      "training_accuracy for epoch  104  -->  0.82490444\n",
      "testing_accuracy for epoch  104  -->  0.81336087\n",
      "\n",
      "\n",
      "loss for epoch  105  -->  0.21328\n",
      "training_accuracy for epoch  105  -->  0.82451105\n",
      "testing_accuracy for epoch  105  -->  0.8170168\n",
      "\n",
      "\n",
      "loss for epoch  106  -->  0.21328\n",
      "training_accuracy for epoch  106  -->  0.82411754\n",
      "testing_accuracy for epoch  106  -->  0.81649446\n",
      "\n",
      "\n",
      "loss for epoch  107  -->  0.21328\n",
      "training_accuracy for epoch  107  -->  0.82490456\n",
      "testing_accuracy for epoch  107  -->  0.81649446\n",
      "\n",
      "\n",
      "loss for epoch  108  -->  0.21328\n",
      "training_accuracy for epoch  108  -->  0.82542926\n",
      "testing_accuracy for epoch  108  -->  0.81649446\n",
      "\n",
      "\n",
      "loss for epoch  109  -->  0.21328\n",
      "training_accuracy for epoch  109  -->  0.82713443\n",
      "testing_accuracy for epoch  109  -->  0.8201504\n",
      "\n",
      "\n",
      "loss for epoch  110  -->  0.21328\n",
      "training_accuracy for epoch  110  -->  0.82490456\n",
      "testing_accuracy for epoch  110  -->  0.8170168\n",
      "\n",
      "\n",
      "loss for epoch  111  -->  0.21328\n",
      "training_accuracy for epoch  111  -->  0.8250357\n",
      "testing_accuracy for epoch  111  -->  0.81649446\n",
      "\n",
      "\n",
      "loss for epoch  112  -->  0.21328\n",
      "training_accuracy for epoch  112  -->  0.8259539\n",
      "testing_accuracy for epoch  112  -->  0.81753904\n",
      "\n",
      "\n",
      "loss for epoch  113  -->  0.21328\n",
      "training_accuracy for epoch  113  -->  0.8276591\n",
      "testing_accuracy for epoch  113  -->  0.8180613\n",
      "\n",
      "\n",
      "loss for epoch  114  -->  0.21328\n",
      "training_accuracy for epoch  114  -->  0.82490456\n",
      "testing_accuracy for epoch  114  -->  0.81544995\n",
      "\n",
      "\n",
      "loss for epoch  115  -->  0.21328\n",
      "training_accuracy for epoch  115  -->  0.8266098\n",
      "testing_accuracy for epoch  115  -->  0.8180613\n",
      "\n",
      "\n",
      "loss for epoch  116  -->  0.21328\n",
      "training_accuracy for epoch  116  -->  0.82700324\n",
      "testing_accuracy for epoch  116  -->  0.8201504\n",
      "\n",
      "\n",
      "loss for epoch  117  -->  0.21328\n",
      "training_accuracy for epoch  117  -->  0.8256915\n",
      "testing_accuracy for epoch  117  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  118  -->  0.21328\n",
      "training_accuracy for epoch  118  -->  0.82608515\n",
      "testing_accuracy for epoch  118  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  119  -->  0.21328\n",
      "training_accuracy for epoch  119  -->  0.8255604\n",
      "testing_accuracy for epoch  119  -->  0.8170168\n",
      "\n",
      "\n",
      "loss for epoch  120  -->  0.21328\n",
      "training_accuracy for epoch  120  -->  0.82765913\n",
      "testing_accuracy for epoch  120  -->  0.8201504\n",
      "\n",
      "\n",
      "loss for epoch  121  -->  0.21328\n",
      "training_accuracy for epoch  121  -->  0.8263474\n",
      "testing_accuracy for epoch  121  -->  0.81858355\n",
      "\n",
      "\n",
      "loss for epoch  122  -->  0.21328\n",
      "training_accuracy for epoch  122  -->  0.82700324\n",
      "testing_accuracy for epoch  122  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  123  -->  0.21328\n",
      "training_accuracy for epoch  123  -->  0.8266097\n",
      "testing_accuracy for epoch  123  -->  0.81753904\n",
      "\n",
      "\n",
      "loss for epoch  124  -->  0.21328\n",
      "training_accuracy for epoch  124  -->  0.82779026\n",
      "testing_accuracy for epoch  124  -->  0.8196281\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for epoch  125  -->  0.21328\n",
      "training_accuracy for epoch  125  -->  0.826085\n",
      "testing_accuracy for epoch  125  -->  0.8191058\n",
      "\n",
      "\n",
      "loss for epoch  126  -->  0.21328\n",
      "training_accuracy for epoch  126  -->  0.82556045\n",
      "testing_accuracy for epoch  126  -->  0.81649446\n",
      "\n",
      "\n",
      "loss for epoch  127  -->  0.21328\n",
      "training_accuracy for epoch  127  -->  0.82556033\n",
      "testing_accuracy for epoch  127  -->  0.81544995\n",
      "\n",
      "\n",
      "loss for epoch  128  -->  0.21328\n",
      "training_accuracy for epoch  128  -->  0.8285773\n",
      "testing_accuracy for epoch  128  -->  0.8149277\n",
      "\n",
      "\n",
      "loss for epoch  129  -->  0.21328\n",
      "training_accuracy for epoch  129  -->  0.8251669\n",
      "testing_accuracy for epoch  129  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  130  -->  0.21328\n",
      "training_accuracy for epoch  130  -->  0.82647854\n",
      "testing_accuracy for epoch  130  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  131  -->  0.21328\n",
      "training_accuracy for epoch  131  -->  0.8252981\n",
      "testing_accuracy for epoch  131  -->  0.81544995\n",
      "\n",
      "\n",
      "loss for epoch  132  -->  0.21328\n",
      "training_accuracy for epoch  132  -->  0.82739675\n",
      "testing_accuracy for epoch  132  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  133  -->  0.21328\n",
      "training_accuracy for epoch  133  -->  0.82608515\n",
      "testing_accuracy for epoch  133  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  134  -->  0.21328\n",
      "training_accuracy for epoch  134  -->  0.8262162\n",
      "testing_accuracy for epoch  134  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  135  -->  0.21328\n",
      "training_accuracy for epoch  135  -->  0.8256915\n",
      "testing_accuracy for epoch  135  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  136  -->  0.21328\n",
      "training_accuracy for epoch  136  -->  0.8267409\n",
      "testing_accuracy for epoch  136  -->  0.81858355\n",
      "\n",
      "\n",
      "loss for epoch  137  -->  0.21328\n",
      "training_accuracy for epoch  137  -->  0.8291019\n",
      "testing_accuracy for epoch  137  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  138  -->  0.21328\n",
      "training_accuracy for epoch  138  -->  0.8283149\n",
      "testing_accuracy for epoch  138  -->  0.81649446\n",
      "\n",
      "\n",
      "loss for epoch  139  -->  0.21328\n",
      "training_accuracy for epoch  139  -->  0.82529795\n",
      "testing_accuracy for epoch  139  -->  0.8191058\n",
      "\n",
      "\n",
      "loss for epoch  140  -->  0.21328\n",
      "training_accuracy for epoch  140  -->  0.82844615\n",
      "testing_accuracy for epoch  140  -->  0.8201504\n",
      "\n",
      "\n",
      "loss for epoch  141  -->  0.21328\n",
      "training_accuracy for epoch  141  -->  0.8275279\n",
      "testing_accuracy for epoch  141  -->  0.8170168\n",
      "\n",
      "\n",
      "loss for epoch  142  -->  0.21328\n",
      "training_accuracy for epoch  142  -->  0.82608515\n",
      "testing_accuracy for epoch  142  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  143  -->  0.21328\n",
      "training_accuracy for epoch  143  -->  0.82529795\n",
      "testing_accuracy for epoch  143  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  144  -->  0.21328\n",
      "training_accuracy for epoch  144  -->  0.8262162\n",
      "testing_accuracy for epoch  144  -->  0.81649446\n",
      "\n",
      "\n",
      "loss for epoch  145  -->  0.21328\n",
      "training_accuracy for epoch  145  -->  0.8284461\n",
      "testing_accuracy for epoch  145  -->  0.8191058\n",
      "\n",
      "\n",
      "loss for epoch  146  -->  0.21328\n",
      "training_accuracy for epoch  146  -->  0.8263473\n",
      "testing_accuracy for epoch  146  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  147  -->  0.21328\n",
      "training_accuracy for epoch  147  -->  0.8256915\n",
      "testing_accuracy for epoch  147  -->  0.81858355\n",
      "\n",
      "\n",
      "loss for epoch  148  -->  0.21328\n",
      "training_accuracy for epoch  148  -->  0.82792133\n",
      "testing_accuracy for epoch  148  -->  0.8191058\n",
      "\n",
      "\n",
      "loss for epoch  149  -->  0.21328\n",
      "training_accuracy for epoch  149  -->  0.8267409\n",
      "testing_accuracy for epoch  149  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  150  -->  0.21328\n",
      "training_accuracy for epoch  150  -->  0.8283149\n",
      "testing_accuracy for epoch  150  -->  0.8201504\n",
      "\n",
      "\n",
      "loss for epoch  151  -->  0.21328\n",
      "training_accuracy for epoch  151  -->  0.8280526\n",
      "testing_accuracy for epoch  151  -->  0.81858355\n",
      "\n",
      "\n",
      "loss for epoch  152  -->  0.21328\n",
      "training_accuracy for epoch  152  -->  0.82713443\n",
      "testing_accuracy for epoch  152  -->  0.81544995\n",
      "\n",
      "\n",
      "loss for epoch  153  -->  0.21328\n",
      "training_accuracy for epoch  153  -->  0.8281837\n",
      "testing_accuracy for epoch  153  -->  0.81753904\n",
      "\n",
      "\n",
      "loss for epoch  154  -->  0.21328\n",
      "training_accuracy for epoch  154  -->  0.8263473\n",
      "testing_accuracy for epoch  154  -->  0.8138831\n",
      "\n",
      "\n",
      "loss for epoch  155  -->  0.21328\n",
      "training_accuracy for epoch  155  -->  0.82936424\n",
      "testing_accuracy for epoch  155  -->  0.8191058\n",
      "\n",
      "\n",
      "loss for epoch  156  -->  0.21328\n",
      "training_accuracy for epoch  156  -->  0.82779026\n",
      "testing_accuracy for epoch  156  -->  0.8180613\n",
      "\n",
      "\n",
      "loss for epoch  157  -->  0.21328\n",
      "training_accuracy for epoch  157  -->  0.8279215\n",
      "testing_accuracy for epoch  157  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  158  -->  0.21328\n",
      "training_accuracy for epoch  158  -->  0.82844603\n",
      "testing_accuracy for epoch  158  -->  0.81440544\n",
      "\n",
      "\n",
      "loss for epoch  159  -->  0.21328\n",
      "training_accuracy for epoch  159  -->  0.82779026\n",
      "testing_accuracy for epoch  159  -->  0.8191058\n",
      "\n",
      "\n",
      "loss for epoch  160  -->  0.21328\n",
      "training_accuracy for epoch  160  -->  0.8281838\n",
      "testing_accuracy for epoch  160  -->  0.81753904\n",
      "\n",
      "\n",
      "loss for epoch  161  -->  0.21328\n",
      "training_accuracy for epoch  161  -->  0.83054477\n",
      "testing_accuracy for epoch  161  -->  0.8191058\n",
      "\n",
      "\n",
      "loss for epoch  162  -->  0.21328\n",
      "training_accuracy for epoch  162  -->  0.82779026\n",
      "testing_accuracy for epoch  162  -->  0.8159722\n",
      "\n",
      "\n",
      "loss for epoch  163  -->  0.21328\n",
      "training_accuracy for epoch  163  -->  0.8289707\n",
      "testing_accuracy for epoch  163  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  164  -->  0.21328\n",
      "training_accuracy for epoch  164  -->  0.82765913\n",
      "testing_accuracy for epoch  164  -->  0.8201504\n",
      "\n",
      "\n",
      "loss for epoch  165  -->  0.21328\n",
      "training_accuracy for epoch  165  -->  0.8289707\n",
      "testing_accuracy for epoch  165  -->  0.8170168\n",
      "\n",
      "\n",
      "loss for epoch  166  -->  0.21328\n",
      "training_accuracy for epoch  166  -->  0.82923317\n",
      "testing_accuracy for epoch  166  -->  0.81858355\n",
      "\n",
      "\n",
      "loss for epoch  167  -->  0.21328\n",
      "training_accuracy for epoch  167  -->  0.8279214\n",
      "testing_accuracy for epoch  167  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  168  -->  0.21328\n",
      "training_accuracy for epoch  168  -->  0.82792133\n",
      "testing_accuracy for epoch  168  -->  0.81753904\n",
      "\n",
      "\n",
      "loss for epoch  169  -->  0.21328\n",
      "training_accuracy for epoch  169  -->  0.8279214\n",
      "testing_accuracy for epoch  169  -->  0.81649446\n",
      "\n",
      "\n",
      "loss for epoch  170  -->  0.21328\n",
      "training_accuracy for epoch  170  -->  0.82988894\n",
      "testing_accuracy for epoch  170  -->  0.81858355\n",
      "\n",
      "\n",
      "loss for epoch  171  -->  0.21328\n",
      "training_accuracy for epoch  171  -->  0.8287084\n",
      "testing_accuracy for epoch  171  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  172  -->  0.21328\n",
      "training_accuracy for epoch  172  -->  0.83067596\n",
      "testing_accuracy for epoch  172  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  173  -->  0.21328\n",
      "training_accuracy for epoch  173  -->  0.826872\n",
      "testing_accuracy for epoch  173  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  174  -->  0.21328\n",
      "training_accuracy for epoch  174  -->  0.8283149\n",
      "testing_accuracy for epoch  174  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  175  -->  0.21328\n",
      "training_accuracy for epoch  175  -->  0.82726556\n",
      "testing_accuracy for epoch  175  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  176  -->  0.21328\n",
      "training_accuracy for epoch  176  -->  0.83106947\n",
      "testing_accuracy for epoch  176  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  177  -->  0.21328\n",
      "training_accuracy for epoch  177  -->  0.8273968\n",
      "testing_accuracy for epoch  177  -->  0.81858355\n",
      "\n",
      "\n",
      "loss for epoch  178  -->  0.21328\n",
      "training_accuracy for epoch  178  -->  0.8279215\n",
      "testing_accuracy for epoch  178  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  179  -->  0.21328\n",
      "training_accuracy for epoch  179  -->  0.83185655\n",
      "testing_accuracy for epoch  179  -->  0.8170168\n",
      "\n",
      "\n",
      "loss for epoch  180  -->  0.21328\n",
      "training_accuracy for epoch  180  -->  0.82910186\n",
      "testing_accuracy for epoch  180  -->  0.81858355\n",
      "\n",
      "\n",
      "loss for epoch  181  -->  0.21328\n",
      "training_accuracy for epoch  181  -->  0.82883966\n",
      "testing_accuracy for epoch  181  -->  0.8201504\n",
      "\n",
      "\n",
      "loss for epoch  182  -->  0.21328\n",
      "training_accuracy for epoch  182  -->  0.8294954\n",
      "testing_accuracy for epoch  182  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  183  -->  0.21328\n",
      "training_accuracy for epoch  183  -->  0.8289708\n",
      "testing_accuracy for epoch  183  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  184  -->  0.21328\n",
      "training_accuracy for epoch  184  -->  0.8291019\n",
      "testing_accuracy for epoch  184  -->  0.81858355\n",
      "\n",
      "\n",
      "loss for epoch  185  -->  0.21328\n",
      "training_accuracy for epoch  185  -->  0.8294954\n",
      "testing_accuracy for epoch  185  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  186  -->  0.21328\n",
      "training_accuracy for epoch  186  -->  0.8283149\n",
      "testing_accuracy for epoch  186  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  187  -->  0.21328\n",
      "training_accuracy for epoch  187  -->  0.8294955\n",
      "testing_accuracy for epoch  187  -->  0.82067263\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss for epoch  188  -->  0.21328\n",
      "training_accuracy for epoch  188  -->  0.82949543\n",
      "testing_accuracy for epoch  188  -->  0.8191058\n",
      "\n",
      "\n",
      "loss for epoch  189  -->  0.21328\n",
      "training_accuracy for epoch  189  -->  0.82923305\n",
      "testing_accuracy for epoch  189  -->  0.81858355\n",
      "\n",
      "\n",
      "loss for epoch  190  -->  0.21328\n",
      "training_accuracy for epoch  190  -->  0.82870847\n",
      "testing_accuracy for epoch  190  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  191  -->  0.21328\n",
      "training_accuracy for epoch  191  -->  0.8284461\n",
      "testing_accuracy for epoch  191  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  192  -->  0.21328\n",
      "training_accuracy for epoch  192  -->  0.83041364\n",
      "testing_accuracy for epoch  192  -->  0.8191058\n",
      "\n",
      "\n",
      "loss for epoch  193  -->  0.21328\n",
      "training_accuracy for epoch  193  -->  0.83015126\n",
      "testing_accuracy for epoch  193  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  194  -->  0.21328\n",
      "training_accuracy for epoch  194  -->  0.83120066\n",
      "testing_accuracy for epoch  194  -->  0.8269399\n",
      "\n",
      "\n",
      "loss for epoch  195  -->  0.21328\n",
      "training_accuracy for epoch  195  -->  0.8308072\n",
      "testing_accuracy for epoch  195  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  196  -->  0.21328\n",
      "training_accuracy for epoch  196  -->  0.8306759\n",
      "testing_accuracy for epoch  196  -->  0.81858355\n",
      "\n",
      "\n",
      "loss for epoch  197  -->  0.21328\n",
      "training_accuracy for epoch  197  -->  0.8297577\n",
      "testing_accuracy for epoch  197  -->  0.8180613\n",
      "\n",
      "\n",
      "loss for epoch  198  -->  0.21328\n",
      "training_accuracy for epoch  198  -->  0.8300202\n",
      "testing_accuracy for epoch  198  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  199  -->  0.21328\n",
      "training_accuracy for epoch  199  -->  0.8305447\n",
      "testing_accuracy for epoch  199  -->  0.81858355\n",
      "\n",
      "\n",
      "loss for epoch  200  -->  0.21328\n",
      "training_accuracy for epoch  200  -->  0.83120066\n",
      "testing_accuracy for epoch  200  -->  0.81753904\n",
      "\n",
      "\n",
      "loss for epoch  201  -->  0.21328\n",
      "training_accuracy for epoch  201  -->  0.8309383\n",
      "testing_accuracy for epoch  201  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  202  -->  0.21328\n",
      "training_accuracy for epoch  202  -->  0.82949543\n",
      "testing_accuracy for epoch  202  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  203  -->  0.21328\n",
      "training_accuracy for epoch  203  -->  0.8315941\n",
      "testing_accuracy for epoch  203  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  204  -->  0.21328\n",
      "training_accuracy for epoch  204  -->  0.83015126\n",
      "testing_accuracy for epoch  204  -->  0.81753904\n",
      "\n",
      "\n",
      "loss for epoch  205  -->  0.21328\n",
      "training_accuracy for epoch  205  -->  0.83185655\n",
      "testing_accuracy for epoch  205  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  206  -->  0.21328\n",
      "training_accuracy for epoch  206  -->  0.8317253\n",
      "testing_accuracy for epoch  206  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  207  -->  0.21328\n",
      "training_accuracy for epoch  207  -->  0.83133185\n",
      "testing_accuracy for epoch  207  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  208  -->  0.21328\n",
      "training_accuracy for epoch  208  -->  0.8321188\n",
      "testing_accuracy for epoch  208  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  209  -->  0.21328\n",
      "training_accuracy for epoch  209  -->  0.83041364\n",
      "testing_accuracy for epoch  209  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  210  -->  0.21328\n",
      "training_accuracy for epoch  210  -->  0.82949543\n",
      "testing_accuracy for epoch  210  -->  0.81649446\n",
      "\n",
      "\n",
      "loss for epoch  211  -->  0.21328\n",
      "training_accuracy for epoch  211  -->  0.8308072\n",
      "testing_accuracy for epoch  211  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  212  -->  0.21328\n",
      "training_accuracy for epoch  212  -->  0.8310694\n",
      "testing_accuracy for epoch  212  -->  0.8191058\n",
      "\n",
      "\n",
      "loss for epoch  213  -->  0.21328\n",
      "training_accuracy for epoch  213  -->  0.82975775\n",
      "testing_accuracy for epoch  213  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  214  -->  0.21328\n",
      "training_accuracy for epoch  214  -->  0.8296266\n",
      "testing_accuracy for epoch  214  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  215  -->  0.21328\n",
      "training_accuracy for epoch  215  -->  0.83133185\n",
      "testing_accuracy for epoch  215  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  216  -->  0.21328\n",
      "training_accuracy for epoch  216  -->  0.8318565\n",
      "testing_accuracy for epoch  216  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  217  -->  0.21328\n",
      "training_accuracy for epoch  217  -->  0.83106947\n",
      "testing_accuracy for epoch  217  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  218  -->  0.21328\n",
      "training_accuracy for epoch  218  -->  0.83225\n",
      "testing_accuracy for epoch  218  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  219  -->  0.21328\n",
      "training_accuracy for epoch  219  -->  0.8317253\n",
      "testing_accuracy for epoch  219  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  220  -->  0.21328\n",
      "training_accuracy for epoch  220  -->  0.83015126\n",
      "testing_accuracy for epoch  220  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  221  -->  0.21328\n",
      "training_accuracy for epoch  221  -->  0.83080715\n",
      "testing_accuracy for epoch  221  -->  0.8170168\n",
      "\n",
      "\n",
      "loss for epoch  222  -->  0.21328\n",
      "training_accuracy for epoch  222  -->  0.83159405\n",
      "testing_accuracy for epoch  222  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  223  -->  0.21328\n",
      "training_accuracy for epoch  223  -->  0.831463\n",
      "testing_accuracy for epoch  223  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  224  -->  0.21328\n",
      "training_accuracy for epoch  224  -->  0.83264357\n",
      "testing_accuracy for epoch  224  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  225  -->  0.21328\n",
      "training_accuracy for epoch  225  -->  0.8309383\n",
      "testing_accuracy for epoch  225  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  226  -->  0.21328\n",
      "training_accuracy for epoch  226  -->  0.83041364\n",
      "testing_accuracy for epoch  226  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  227  -->  0.21328\n",
      "training_accuracy for epoch  227  -->  0.8305448\n",
      "testing_accuracy for epoch  227  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  228  -->  0.21328\n",
      "training_accuracy for epoch  228  -->  0.83054477\n",
      "testing_accuracy for epoch  228  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  229  -->  0.21328\n",
      "training_accuracy for epoch  229  -->  0.8309383\n",
      "testing_accuracy for epoch  229  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  230  -->  0.21328\n",
      "training_accuracy for epoch  230  -->  0.83159405\n",
      "testing_accuracy for epoch  230  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  231  -->  0.21328\n",
      "training_accuracy for epoch  231  -->  0.8313318\n",
      "testing_accuracy for epoch  231  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  232  -->  0.21328\n",
      "training_accuracy for epoch  232  -->  0.8317253\n",
      "testing_accuracy for epoch  232  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  233  -->  0.21328\n",
      "training_accuracy for epoch  233  -->  0.831463\n",
      "testing_accuracy for epoch  233  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  234  -->  0.21328\n",
      "training_accuracy for epoch  234  -->  0.8309383\n",
      "testing_accuracy for epoch  234  -->  0.8180613\n",
      "\n",
      "\n",
      "loss for epoch  235  -->  0.21328\n",
      "training_accuracy for epoch  235  -->  0.8300202\n",
      "testing_accuracy for epoch  235  -->  0.8191058\n",
      "\n",
      "\n",
      "loss for epoch  236  -->  0.21328\n",
      "training_accuracy for epoch  236  -->  0.8315942\n",
      "testing_accuracy for epoch  236  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  237  -->  0.21328\n",
      "training_accuracy for epoch  237  -->  0.8305447\n",
      "testing_accuracy for epoch  237  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  238  -->  0.21328\n",
      "training_accuracy for epoch  238  -->  0.83159405\n",
      "testing_accuracy for epoch  238  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  239  -->  0.21328\n",
      "training_accuracy for epoch  239  -->  0.8319876\n",
      "testing_accuracy for epoch  239  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  240  -->  0.21328\n",
      "training_accuracy for epoch  240  -->  0.8321188\n",
      "testing_accuracy for epoch  240  -->  0.8201504\n",
      "\n",
      "\n",
      "loss for epoch  241  -->  0.21328\n",
      "training_accuracy for epoch  241  -->  0.83329934\n",
      "testing_accuracy for epoch  241  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  242  -->  0.21328\n",
      "training_accuracy for epoch  242  -->  0.8323811\n",
      "testing_accuracy for epoch  242  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  243  -->  0.21328\n",
      "training_accuracy for epoch  243  -->  0.83290577\n",
      "testing_accuracy for epoch  243  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  244  -->  0.21328\n",
      "training_accuracy for epoch  244  -->  0.8305447\n",
      "testing_accuracy for epoch  244  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  245  -->  0.21328\n",
      "training_accuracy for epoch  245  -->  0.833037\n",
      "testing_accuracy for epoch  245  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  246  -->  0.21328\n",
      "training_accuracy for epoch  246  -->  0.83067596\n",
      "testing_accuracy for epoch  246  -->  0.8180613\n",
      "\n",
      "\n",
      "loss for epoch  247  -->  0.21328\n",
      "training_accuracy for epoch  247  -->  0.831463\n",
      "testing_accuracy for epoch  247  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  248  -->  0.21328\n",
      "training_accuracy for epoch  248  -->  0.83369285\n",
      "testing_accuracy for epoch  248  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  249  -->  0.21328\n",
      "training_accuracy for epoch  249  -->  0.83120066\n",
      "testing_accuracy for epoch  249  -->  0.81753904\n",
      "\n",
      "\n",
      "loss for epoch  250  -->  0.21328\n",
      "training_accuracy for epoch  250  -->  0.8329058\n",
      "testing_accuracy for epoch  250  -->  0.82067263\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for epoch  251  -->  0.21328\n",
      "training_accuracy for epoch  251  -->  0.83238107\n",
      "testing_accuracy for epoch  251  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  252  -->  0.21328\n",
      "training_accuracy for epoch  252  -->  0.833037\n",
      "testing_accuracy for epoch  252  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  253  -->  0.21328\n",
      "training_accuracy for epoch  253  -->  0.83133173\n",
      "testing_accuracy for epoch  253  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  254  -->  0.21328\n",
      "training_accuracy for epoch  254  -->  0.8323811\n",
      "testing_accuracy for epoch  254  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  255  -->  0.21328\n",
      "training_accuracy for epoch  255  -->  0.831463\n",
      "testing_accuracy for epoch  255  -->  0.8191058\n",
      "\n",
      "\n",
      "loss for epoch  256  -->  0.21328\n",
      "training_accuracy for epoch  256  -->  0.8313318\n",
      "testing_accuracy for epoch  256  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  257  -->  0.21328\n",
      "training_accuracy for epoch  257  -->  0.83369285\n",
      "testing_accuracy for epoch  257  -->  0.8170168\n",
      "\n",
      "\n",
      "loss for epoch  258  -->  0.21328\n",
      "training_accuracy for epoch  258  -->  0.83093834\n",
      "testing_accuracy for epoch  258  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  259  -->  0.21328\n",
      "training_accuracy for epoch  259  -->  0.8319876\n",
      "testing_accuracy for epoch  259  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  260  -->  0.21328\n",
      "training_accuracy for epoch  260  -->  0.831463\n",
      "testing_accuracy for epoch  260  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  261  -->  0.21328\n",
      "training_accuracy for epoch  261  -->  0.8325123\n",
      "testing_accuracy for epoch  261  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  262  -->  0.21328\n",
      "training_accuracy for epoch  262  -->  0.833037\n",
      "testing_accuracy for epoch  262  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  263  -->  0.21328\n",
      "training_accuracy for epoch  263  -->  0.83290577\n",
      "testing_accuracy for epoch  263  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  264  -->  0.21328\n",
      "training_accuracy for epoch  264  -->  0.8315941\n",
      "testing_accuracy for epoch  264  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  265  -->  0.21328\n",
      "training_accuracy for epoch  265  -->  0.83316815\n",
      "testing_accuracy for epoch  265  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  266  -->  0.21328\n",
      "training_accuracy for epoch  266  -->  0.833037\n",
      "testing_accuracy for epoch  266  -->  0.8258953\n",
      "\n",
      "\n",
      "loss for epoch  267  -->  0.21328\n",
      "training_accuracy for epoch  267  -->  0.8325123\n",
      "testing_accuracy for epoch  267  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  268  -->  0.21328\n",
      "training_accuracy for epoch  268  -->  0.83343047\n",
      "testing_accuracy for epoch  268  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  269  -->  0.21328\n",
      "training_accuracy for epoch  269  -->  0.834611\n",
      "testing_accuracy for epoch  269  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  270  -->  0.21328\n",
      "training_accuracy for epoch  270  -->  0.8347421\n",
      "testing_accuracy for epoch  270  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  271  -->  0.21328\n",
      "training_accuracy for epoch  271  -->  0.8342176\n",
      "testing_accuracy for epoch  271  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  272  -->  0.21328\n",
      "training_accuracy for epoch  272  -->  0.8323811\n",
      "testing_accuracy for epoch  272  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  273  -->  0.21328\n",
      "training_accuracy for epoch  273  -->  0.8319876\n",
      "testing_accuracy for epoch  273  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  274  -->  0.21328\n",
      "training_accuracy for epoch  274  -->  0.8325123\n",
      "testing_accuracy for epoch  274  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  275  -->  0.21328\n",
      "training_accuracy for epoch  275  -->  0.8317253\n",
      "testing_accuracy for epoch  275  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  276  -->  0.21328\n",
      "training_accuracy for epoch  276  -->  0.83356166\n",
      "testing_accuracy for epoch  276  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  277  -->  0.21328\n",
      "training_accuracy for epoch  277  -->  0.8329058\n",
      "testing_accuracy for epoch  277  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  278  -->  0.21328\n",
      "training_accuracy for epoch  278  -->  0.8352668\n",
      "testing_accuracy for epoch  278  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  279  -->  0.21328\n",
      "training_accuracy for epoch  279  -->  0.83329934\n",
      "testing_accuracy for epoch  279  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  280  -->  0.21328\n",
      "training_accuracy for epoch  280  -->  0.8325123\n",
      "testing_accuracy for epoch  280  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  281  -->  0.21328\n",
      "training_accuracy for epoch  281  -->  0.83316815\n",
      "testing_accuracy for epoch  281  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  282  -->  0.21328\n",
      "training_accuracy for epoch  282  -->  0.83356166\n",
      "testing_accuracy for epoch  282  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  283  -->  0.21328\n",
      "training_accuracy for epoch  283  -->  0.833824\n",
      "testing_accuracy for epoch  283  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  284  -->  0.21328\n",
      "training_accuracy for epoch  284  -->  0.8309383\n",
      "testing_accuracy for epoch  284  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  285  -->  0.21328\n",
      "training_accuracy for epoch  285  -->  0.83329934\n",
      "testing_accuracy for epoch  285  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  286  -->  0.21328\n",
      "training_accuracy for epoch  286  -->  0.8347421\n",
      "testing_accuracy for epoch  286  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  287  -->  0.21328\n",
      "training_accuracy for epoch  287  -->  0.8331681\n",
      "testing_accuracy for epoch  287  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  288  -->  0.21328\n",
      "training_accuracy for epoch  288  -->  0.8350045\n",
      "testing_accuracy for epoch  288  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  289  -->  0.21328\n",
      "training_accuracy for epoch  289  -->  0.8325123\n",
      "testing_accuracy for epoch  289  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  290  -->  0.21328\n",
      "training_accuracy for epoch  290  -->  0.8343487\n",
      "testing_accuracy for epoch  290  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  291  -->  0.21328\n",
      "training_accuracy for epoch  291  -->  0.8334306\n",
      "testing_accuracy for epoch  291  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  292  -->  0.21328\n",
      "training_accuracy for epoch  292  -->  0.8342174\n",
      "testing_accuracy for epoch  292  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  293  -->  0.21328\n",
      "training_accuracy for epoch  293  -->  0.8338241\n",
      "testing_accuracy for epoch  293  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  294  -->  0.21328\n",
      "training_accuracy for epoch  294  -->  0.833824\n",
      "testing_accuracy for epoch  294  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  295  -->  0.21328\n",
      "training_accuracy for epoch  295  -->  0.83369285\n",
      "testing_accuracy for epoch  295  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  296  -->  0.21328\n",
      "training_accuracy for epoch  296  -->  0.8344799\n",
      "testing_accuracy for epoch  296  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  297  -->  0.21328\n",
      "training_accuracy for epoch  297  -->  0.8348734\n",
      "testing_accuracy for epoch  297  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  298  -->  0.21328\n",
      "training_accuracy for epoch  298  -->  0.8351357\n",
      "testing_accuracy for epoch  298  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  299  -->  0.21328\n",
      "training_accuracy for epoch  299  -->  0.8342174\n",
      "testing_accuracy for epoch  299  -->  0.8201504\n",
      "\n",
      "\n",
      "loss for epoch  300  -->  0.21328\n",
      "training_accuracy for epoch  300  -->  0.8352668\n",
      "testing_accuracy for epoch  300  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  301  -->  0.21328\n",
      "training_accuracy for epoch  301  -->  0.8339551\n",
      "testing_accuracy for epoch  301  -->  0.8201504\n",
      "\n",
      "\n",
      "loss for epoch  302  -->  0.21328\n",
      "training_accuracy for epoch  302  -->  0.83356166\n",
      "testing_accuracy for epoch  302  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  303  -->  0.21328\n",
      "training_accuracy for epoch  303  -->  0.8351357\n",
      "testing_accuracy for epoch  303  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  304  -->  0.21328\n",
      "training_accuracy for epoch  304  -->  0.8331682\n",
      "testing_accuracy for epoch  304  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  305  -->  0.21328\n",
      "training_accuracy for epoch  305  -->  0.8329059\n",
      "testing_accuracy for epoch  305  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  306  -->  0.21328\n",
      "training_accuracy for epoch  306  -->  0.83395517\n",
      "testing_accuracy for epoch  306  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  307  -->  0.21328\n",
      "training_accuracy for epoch  307  -->  0.833037\n",
      "testing_accuracy for epoch  307  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  308  -->  0.21328\n",
      "training_accuracy for epoch  308  -->  0.8348734\n",
      "testing_accuracy for epoch  308  -->  0.8201504\n",
      "\n",
      "\n",
      "loss for epoch  309  -->  0.21328\n",
      "training_accuracy for epoch  309  -->  0.8347422\n",
      "testing_accuracy for epoch  309  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  310  -->  0.21328\n",
      "training_accuracy for epoch  310  -->  0.83526677\n",
      "testing_accuracy for epoch  310  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  311  -->  0.21328\n",
      "training_accuracy for epoch  311  -->  0.8342176\n",
      "testing_accuracy for epoch  311  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  312  -->  0.21328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy for epoch  312  -->  0.8336929\n",
      "testing_accuracy for epoch  312  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  313  -->  0.21328\n",
      "training_accuracy for epoch  313  -->  0.83408636\n",
      "testing_accuracy for epoch  313  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  314  -->  0.21328\n",
      "training_accuracy for epoch  314  -->  0.8342175\n",
      "testing_accuracy for epoch  314  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  315  -->  0.21328\n",
      "training_accuracy for epoch  315  -->  0.8342176\n",
      "testing_accuracy for epoch  315  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  316  -->  0.21328\n",
      "training_accuracy for epoch  316  -->  0.83316815\n",
      "testing_accuracy for epoch  316  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  317  -->  0.21328\n",
      "training_accuracy for epoch  317  -->  0.83369285\n",
      "testing_accuracy for epoch  317  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  318  -->  0.21328\n",
      "training_accuracy for epoch  318  -->  0.83447987\n",
      "testing_accuracy for epoch  318  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  319  -->  0.21328\n",
      "training_accuracy for epoch  319  -->  0.8343487\n",
      "testing_accuracy for epoch  319  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  320  -->  0.21328\n",
      "training_accuracy for epoch  320  -->  0.833037\n",
      "testing_accuracy for epoch  320  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  321  -->  0.21328\n",
      "training_accuracy for epoch  321  -->  0.8331681\n",
      "testing_accuracy for epoch  321  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  322  -->  0.21328\n",
      "training_accuracy for epoch  322  -->  0.8348734\n",
      "testing_accuracy for epoch  322  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  323  -->  0.21328\n",
      "training_accuracy for epoch  323  -->  0.83474225\n",
      "testing_accuracy for epoch  323  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  324  -->  0.21328\n",
      "training_accuracy for epoch  324  -->  0.8331681\n",
      "testing_accuracy for epoch  324  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  325  -->  0.21328\n",
      "training_accuracy for epoch  325  -->  0.8347422\n",
      "testing_accuracy for epoch  325  -->  0.8201504\n",
      "\n",
      "\n",
      "loss for epoch  326  -->  0.21328\n",
      "training_accuracy for epoch  326  -->  0.833037\n",
      "testing_accuracy for epoch  326  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  327  -->  0.21328\n",
      "training_accuracy for epoch  327  -->  0.83408636\n",
      "testing_accuracy for epoch  327  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  328  -->  0.21328\n",
      "training_accuracy for epoch  328  -->  0.835398\n",
      "testing_accuracy for epoch  328  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  329  -->  0.21328\n",
      "training_accuracy for epoch  329  -->  0.83225006\n",
      "testing_accuracy for epoch  329  -->  0.8191058\n",
      "\n",
      "\n",
      "loss for epoch  330  -->  0.21328\n",
      "training_accuracy for epoch  330  -->  0.8342174\n",
      "testing_accuracy for epoch  330  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  331  -->  0.21328\n",
      "training_accuracy for epoch  331  -->  0.83395517\n",
      "testing_accuracy for epoch  331  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  332  -->  0.21328\n",
      "training_accuracy for epoch  332  -->  0.8343487\n",
      "testing_accuracy for epoch  332  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  333  -->  0.21328\n",
      "training_accuracy for epoch  333  -->  0.83408636\n",
      "testing_accuracy for epoch  333  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  334  -->  0.21328\n",
      "training_accuracy for epoch  334  -->  0.83566034\n",
      "testing_accuracy for epoch  334  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  335  -->  0.21328\n",
      "training_accuracy for epoch  335  -->  0.834611\n",
      "testing_accuracy for epoch  335  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  336  -->  0.21328\n",
      "training_accuracy for epoch  336  -->  0.83526677\n",
      "testing_accuracy for epoch  336  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  337  -->  0.21328\n",
      "training_accuracy for epoch  337  -->  0.8344799\n",
      "testing_accuracy for epoch  337  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  338  -->  0.21328\n",
      "training_accuracy for epoch  338  -->  0.834611\n",
      "testing_accuracy for epoch  338  -->  0.8201504\n",
      "\n",
      "\n",
      "loss for epoch  339  -->  0.21328\n",
      "training_accuracy for epoch  339  -->  0.835398\n",
      "testing_accuracy for epoch  339  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  340  -->  0.21328\n",
      "training_accuracy for epoch  340  -->  0.83474225\n",
      "testing_accuracy for epoch  340  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  341  -->  0.21328\n",
      "training_accuracy for epoch  341  -->  0.8365785\n",
      "testing_accuracy for epoch  341  -->  0.8196281\n",
      "\n",
      "\n",
      "loss for epoch  342  -->  0.21328\n",
      "training_accuracy for epoch  342  -->  0.83526695\n",
      "testing_accuracy for epoch  342  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  343  -->  0.21328\n",
      "training_accuracy for epoch  343  -->  0.83356166\n",
      "testing_accuracy for epoch  343  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  344  -->  0.21328\n",
      "training_accuracy for epoch  344  -->  0.83526677\n",
      "testing_accuracy for epoch  344  -->  0.8191058\n",
      "\n",
      "\n",
      "loss for epoch  345  -->  0.21328\n",
      "training_accuracy for epoch  345  -->  0.8342174\n",
      "testing_accuracy for epoch  345  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  346  -->  0.21328\n",
      "training_accuracy for epoch  346  -->  0.8355292\n",
      "testing_accuracy for epoch  346  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  347  -->  0.21328\n",
      "training_accuracy for epoch  347  -->  0.83618504\n",
      "testing_accuracy for epoch  347  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  348  -->  0.21328\n",
      "training_accuracy for epoch  348  -->  0.83605385\n",
      "testing_accuracy for epoch  348  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  349  -->  0.21328\n",
      "training_accuracy for epoch  349  -->  0.8336929\n",
      "testing_accuracy for epoch  349  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  350  -->  0.21328\n",
      "training_accuracy for epoch  350  -->  0.83552915\n",
      "testing_accuracy for epoch  350  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  351  -->  0.21328\n",
      "training_accuracy for epoch  351  -->  0.83316815\n",
      "testing_accuracy for epoch  351  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  352  -->  0.21328\n",
      "training_accuracy for epoch  352  -->  0.8339551\n",
      "testing_accuracy for epoch  352  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  353  -->  0.21328\n",
      "training_accuracy for epoch  353  -->  0.8372344\n",
      "testing_accuracy for epoch  353  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  354  -->  0.21328\n",
      "training_accuracy for epoch  354  -->  0.8344799\n",
      "testing_accuracy for epoch  354  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  355  -->  0.21328\n",
      "training_accuracy for epoch  355  -->  0.83408636\n",
      "testing_accuracy for epoch  355  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  356  -->  0.21328\n",
      "training_accuracy for epoch  356  -->  0.8343487\n",
      "testing_accuracy for epoch  356  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  357  -->  0.21328\n",
      "training_accuracy for epoch  357  -->  0.83500457\n",
      "testing_accuracy for epoch  357  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  358  -->  0.21328\n",
      "training_accuracy for epoch  358  -->  0.8359227\n",
      "testing_accuracy for epoch  358  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  359  -->  0.21328\n",
      "training_accuracy for epoch  359  -->  0.83500445\n",
      "testing_accuracy for epoch  359  -->  0.8269399\n",
      "\n",
      "\n",
      "loss for epoch  360  -->  0.21328\n",
      "training_accuracy for epoch  360  -->  0.83566034\n",
      "testing_accuracy for epoch  360  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  361  -->  0.21328\n",
      "training_accuracy for epoch  361  -->  0.8350045\n",
      "testing_accuracy for epoch  361  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  362  -->  0.21328\n",
      "training_accuracy for epoch  362  -->  0.83618504\n",
      "testing_accuracy for epoch  362  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  363  -->  0.21328\n",
      "training_accuracy for epoch  363  -->  0.8355292\n",
      "testing_accuracy for epoch  363  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  364  -->  0.21328\n",
      "training_accuracy for epoch  364  -->  0.8343487\n",
      "testing_accuracy for epoch  364  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  365  -->  0.21328\n",
      "training_accuracy for epoch  365  -->  0.8336928\n",
      "testing_accuracy for epoch  365  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  366  -->  0.21328\n",
      "training_accuracy for epoch  366  -->  0.8359227\n",
      "testing_accuracy for epoch  366  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  367  -->  0.21328\n",
      "training_accuracy for epoch  367  -->  0.83552927\n",
      "testing_accuracy for epoch  367  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  368  -->  0.21328\n",
      "training_accuracy for epoch  368  -->  0.8348734\n",
      "testing_accuracy for epoch  368  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  369  -->  0.21328\n",
      "training_accuracy for epoch  369  -->  0.835398\n",
      "testing_accuracy for epoch  369  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  370  -->  0.21328\n",
      "training_accuracy for epoch  370  -->  0.8344799\n",
      "testing_accuracy for epoch  370  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  371  -->  0.21328\n",
      "training_accuracy for epoch  371  -->  0.83447975\n",
      "testing_accuracy for epoch  371  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  372  -->  0.21328\n",
      "training_accuracy for epoch  372  -->  0.83644736\n",
      "testing_accuracy for epoch  372  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  373  -->  0.21328\n",
      "training_accuracy for epoch  373  -->  0.8350045\n",
      "testing_accuracy for epoch  373  -->  0.82171714\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for epoch  374  -->  0.21328\n",
      "training_accuracy for epoch  374  -->  0.83789015\n",
      "testing_accuracy for epoch  374  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  375  -->  0.21328\n",
      "training_accuracy for epoch  375  -->  0.8359227\n",
      "testing_accuracy for epoch  375  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  376  -->  0.21328\n",
      "training_accuracy for epoch  376  -->  0.8351357\n",
      "testing_accuracy for epoch  376  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  377  -->  0.21328\n",
      "training_accuracy for epoch  377  -->  0.8360538\n",
      "testing_accuracy for epoch  377  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  378  -->  0.21328\n",
      "training_accuracy for epoch  378  -->  0.83697206\n",
      "testing_accuracy for epoch  378  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  379  -->  0.21328\n",
      "training_accuracy for epoch  379  -->  0.83447987\n",
      "testing_accuracy for epoch  379  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  380  -->  0.21328\n",
      "training_accuracy for epoch  380  -->  0.83552915\n",
      "testing_accuracy for epoch  380  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  381  -->  0.21328\n",
      "training_accuracy for epoch  381  -->  0.8359226\n",
      "testing_accuracy for epoch  381  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  382  -->  0.21328\n",
      "training_accuracy for epoch  382  -->  0.83474225\n",
      "testing_accuracy for epoch  382  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  383  -->  0.21328\n",
      "training_accuracy for epoch  383  -->  0.8348734\n",
      "testing_accuracy for epoch  383  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  384  -->  0.21328\n",
      "training_accuracy for epoch  384  -->  0.8351357\n",
      "testing_accuracy for epoch  384  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  385  -->  0.21328\n",
      "training_accuracy for epoch  385  -->  0.83566034\n",
      "testing_accuracy for epoch  385  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  386  -->  0.21328\n",
      "training_accuracy for epoch  386  -->  0.835398\n",
      "testing_accuracy for epoch  386  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  387  -->  0.21328\n",
      "training_accuracy for epoch  387  -->  0.8351357\n",
      "testing_accuracy for epoch  387  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  388  -->  0.21328\n",
      "training_accuracy for epoch  388  -->  0.834611\n",
      "testing_accuracy for epoch  388  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  389  -->  0.21328\n",
      "training_accuracy for epoch  389  -->  0.83500445\n",
      "testing_accuracy for epoch  389  -->  0.82641757\n",
      "\n",
      "\n",
      "loss for epoch  390  -->  0.21328\n",
      "training_accuracy for epoch  390  -->  0.8343487\n",
      "testing_accuracy for epoch  390  -->  0.82746214\n",
      "\n",
      "\n",
      "loss for epoch  391  -->  0.21328\n",
      "training_accuracy for epoch  391  -->  0.83644736\n",
      "testing_accuracy for epoch  391  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  392  -->  0.21328\n",
      "training_accuracy for epoch  392  -->  0.83500457\n",
      "testing_accuracy for epoch  392  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  393  -->  0.21328\n",
      "training_accuracy for epoch  393  -->  0.8355292\n",
      "testing_accuracy for epoch  393  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  394  -->  0.21328\n",
      "training_accuracy for epoch  394  -->  0.83618504\n",
      "testing_accuracy for epoch  394  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  395  -->  0.21328\n",
      "training_accuracy for epoch  395  -->  0.83618504\n",
      "testing_accuracy for epoch  395  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  396  -->  0.21328\n",
      "training_accuracy for epoch  396  -->  0.8363163\n",
      "testing_accuracy for epoch  396  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  397  -->  0.21328\n",
      "training_accuracy for epoch  397  -->  0.8352668\n",
      "testing_accuracy for epoch  397  -->  0.82223946\n",
      "\n",
      "\n",
      "loss for epoch  398  -->  0.21328\n",
      "training_accuracy for epoch  398  -->  0.83447975\n",
      "testing_accuracy for epoch  398  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  399  -->  0.21328\n",
      "training_accuracy for epoch  399  -->  0.8343487\n",
      "testing_accuracy for epoch  399  -->  0.8258953\n",
      "\n",
      "\n",
      "loss for epoch  400  -->  0.21328\n",
      "training_accuracy for epoch  400  -->  0.83539796\n",
      "testing_accuracy for epoch  400  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  401  -->  0.21328\n",
      "training_accuracy for epoch  401  -->  0.83605397\n",
      "testing_accuracy for epoch  401  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  402  -->  0.21328\n",
      "training_accuracy for epoch  402  -->  0.835398\n",
      "testing_accuracy for epoch  402  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  403  -->  0.21328\n",
      "training_accuracy for epoch  403  -->  0.8368408\n",
      "testing_accuracy for epoch  403  -->  0.8290289\n",
      "\n",
      "\n",
      "loss for epoch  404  -->  0.21328\n",
      "training_accuracy for epoch  404  -->  0.835398\n",
      "testing_accuracy for epoch  404  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  405  -->  0.21328\n",
      "training_accuracy for epoch  405  -->  0.83552927\n",
      "testing_accuracy for epoch  405  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  406  -->  0.21328\n",
      "training_accuracy for epoch  406  -->  0.8343487\n",
      "testing_accuracy for epoch  406  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  407  -->  0.21328\n",
      "training_accuracy for epoch  407  -->  0.8359227\n",
      "testing_accuracy for epoch  407  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  408  -->  0.21328\n",
      "training_accuracy for epoch  408  -->  0.83605397\n",
      "testing_accuracy for epoch  408  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  409  -->  0.21328\n",
      "training_accuracy for epoch  409  -->  0.8348734\n",
      "testing_accuracy for epoch  409  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  410  -->  0.21328\n",
      "training_accuracy for epoch  410  -->  0.8355292\n",
      "testing_accuracy for epoch  410  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  411  -->  0.21328\n",
      "training_accuracy for epoch  411  -->  0.83447987\n",
      "testing_accuracy for epoch  411  -->  0.8258953\n",
      "\n",
      "\n",
      "loss for epoch  412  -->  0.21328\n",
      "training_accuracy for epoch  412  -->  0.8352668\n",
      "testing_accuracy for epoch  412  -->  0.8279844\n",
      "\n",
      "\n",
      "loss for epoch  413  -->  0.21328\n",
      "training_accuracy for epoch  413  -->  0.8373656\n",
      "testing_accuracy for epoch  413  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  414  -->  0.21328\n",
      "training_accuracy for epoch  414  -->  0.8363161\n",
      "testing_accuracy for epoch  414  -->  0.82641757\n",
      "\n",
      "\n",
      "loss for epoch  415  -->  0.21328\n",
      "training_accuracy for epoch  415  -->  0.83618504\n",
      "testing_accuracy for epoch  415  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  416  -->  0.21328\n",
      "training_accuracy for epoch  416  -->  0.83684087\n",
      "testing_accuracy for epoch  416  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  417  -->  0.21328\n",
      "training_accuracy for epoch  417  -->  0.83670974\n",
      "testing_accuracy for epoch  417  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  418  -->  0.21328\n",
      "training_accuracy for epoch  418  -->  0.83618504\n",
      "testing_accuracy for epoch  418  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  419  -->  0.21328\n",
      "training_accuracy for epoch  419  -->  0.8364473\n",
      "testing_accuracy for epoch  419  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  420  -->  0.21328\n",
      "training_accuracy for epoch  420  -->  0.8348734\n",
      "testing_accuracy for epoch  420  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  421  -->  0.21328\n",
      "training_accuracy for epoch  421  -->  0.83500457\n",
      "testing_accuracy for epoch  421  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  422  -->  0.21328\n",
      "training_accuracy for epoch  422  -->  0.8343487\n",
      "testing_accuracy for epoch  422  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  423  -->  0.21328\n",
      "training_accuracy for epoch  423  -->  0.8351357\n",
      "testing_accuracy for epoch  423  -->  0.8269399\n",
      "\n",
      "\n",
      "loss for epoch  424  -->  0.21328\n",
      "training_accuracy for epoch  424  -->  0.8372344\n",
      "testing_accuracy for epoch  424  -->  0.8258953\n",
      "\n",
      "\n",
      "loss for epoch  425  -->  0.21328\n",
      "training_accuracy for epoch  425  -->  0.8363162\n",
      "testing_accuracy for epoch  425  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  426  -->  0.21328\n",
      "training_accuracy for epoch  426  -->  0.8363163\n",
      "testing_accuracy for epoch  426  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  427  -->  0.21328\n",
      "training_accuracy for epoch  427  -->  0.8368409\n",
      "testing_accuracy for epoch  427  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  428  -->  0.21328\n",
      "training_accuracy for epoch  428  -->  0.83670974\n",
      "testing_accuracy for epoch  428  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  429  -->  0.21328\n",
      "training_accuracy for epoch  429  -->  0.83697206\n",
      "testing_accuracy for epoch  429  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  430  -->  0.21328\n",
      "training_accuracy for epoch  430  -->  0.83500457\n",
      "testing_accuracy for epoch  430  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  431  -->  0.21328\n",
      "training_accuracy for epoch  431  -->  0.83539796\n",
      "testing_accuracy for epoch  431  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  432  -->  0.21328\n",
      "training_accuracy for epoch  432  -->  0.83526677\n",
      "testing_accuracy for epoch  432  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  433  -->  0.21328\n",
      "training_accuracy for epoch  433  -->  0.834611\n",
      "testing_accuracy for epoch  433  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  434  -->  0.21328\n",
      "training_accuracy for epoch  434  -->  0.8334304\n",
      "testing_accuracy for epoch  434  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  435  -->  0.21328\n",
      "training_accuracy for epoch  435  -->  0.8365786\n",
      "testing_accuracy for epoch  435  -->  0.82537305\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for epoch  436  -->  0.21328\n",
      "training_accuracy for epoch  436  -->  0.83487326\n",
      "testing_accuracy for epoch  436  -->  0.8269399\n",
      "\n",
      "\n",
      "loss for epoch  437  -->  0.21328\n",
      "training_accuracy for epoch  437  -->  0.83605397\n",
      "testing_accuracy for epoch  437  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  438  -->  0.21328\n",
      "training_accuracy for epoch  438  -->  0.8363163\n",
      "testing_accuracy for epoch  438  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  439  -->  0.21328\n",
      "training_accuracy for epoch  439  -->  0.835398\n",
      "testing_accuracy for epoch  439  -->  0.8269399\n",
      "\n",
      "\n",
      "loss for epoch  440  -->  0.21328\n",
      "training_accuracy for epoch  440  -->  0.83618504\n",
      "testing_accuracy for epoch  440  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  441  -->  0.21328\n",
      "training_accuracy for epoch  441  -->  0.8359227\n",
      "testing_accuracy for epoch  441  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  442  -->  0.21328\n",
      "training_accuracy for epoch  442  -->  0.8359227\n",
      "testing_accuracy for epoch  442  -->  0.8201504\n",
      "\n",
      "\n",
      "loss for epoch  443  -->  0.21328\n",
      "training_accuracy for epoch  443  -->  0.83526677\n",
      "testing_accuracy for epoch  443  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  444  -->  0.21328\n",
      "training_accuracy for epoch  444  -->  0.83815247\n",
      "testing_accuracy for epoch  444  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  445  -->  0.21328\n",
      "training_accuracy for epoch  445  -->  0.83500457\n",
      "testing_accuracy for epoch  445  -->  0.8258953\n",
      "\n",
      "\n",
      "loss for epoch  446  -->  0.21328\n",
      "training_accuracy for epoch  446  -->  0.83474225\n",
      "testing_accuracy for epoch  446  -->  0.82641757\n",
      "\n",
      "\n",
      "loss for epoch  447  -->  0.21328\n",
      "training_accuracy for epoch  447  -->  0.8363161\n",
      "testing_accuracy for epoch  447  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  448  -->  0.21328\n",
      "training_accuracy for epoch  448  -->  0.8357915\n",
      "testing_accuracy for epoch  448  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  449  -->  0.21328\n",
      "training_accuracy for epoch  449  -->  0.83552915\n",
      "testing_accuracy for epoch  449  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  450  -->  0.21328\n",
      "training_accuracy for epoch  450  -->  0.83566034\n",
      "testing_accuracy for epoch  450  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  451  -->  0.21328\n",
      "training_accuracy for epoch  451  -->  0.8365786\n",
      "testing_accuracy for epoch  451  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  452  -->  0.21328\n",
      "training_accuracy for epoch  452  -->  0.83618504\n",
      "testing_accuracy for epoch  452  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  453  -->  0.21328\n",
      "training_accuracy for epoch  453  -->  0.83618504\n",
      "testing_accuracy for epoch  453  -->  0.82746214\n",
      "\n",
      "\n",
      "loss for epoch  454  -->  0.21328\n",
      "training_accuracy for epoch  454  -->  0.8364474\n",
      "testing_accuracy for epoch  454  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  455  -->  0.21328\n",
      "training_accuracy for epoch  455  -->  0.8355292\n",
      "testing_accuracy for epoch  455  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  456  -->  0.21328\n",
      "training_accuracy for epoch  456  -->  0.8351357\n",
      "testing_accuracy for epoch  456  -->  0.8269399\n",
      "\n",
      "\n",
      "loss for epoch  457  -->  0.21328\n",
      "training_accuracy for epoch  457  -->  0.8353981\n",
      "testing_accuracy for epoch  457  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  458  -->  0.21328\n",
      "training_accuracy for epoch  458  -->  0.83605385\n",
      "testing_accuracy for epoch  458  -->  0.8258953\n",
      "\n",
      "\n",
      "loss for epoch  459  -->  0.21328\n",
      "training_accuracy for epoch  459  -->  0.8351357\n",
      "testing_accuracy for epoch  459  -->  0.8258953\n",
      "\n",
      "\n",
      "loss for epoch  460  -->  0.21328\n",
      "training_accuracy for epoch  460  -->  0.8336929\n",
      "testing_accuracy for epoch  460  -->  0.8258953\n",
      "\n",
      "\n",
      "loss for epoch  461  -->  0.21328\n",
      "training_accuracy for epoch  461  -->  0.8343487\n",
      "testing_accuracy for epoch  461  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  462  -->  0.21328\n",
      "training_accuracy for epoch  462  -->  0.8351357\n",
      "testing_accuracy for epoch  462  -->  0.8258953\n",
      "\n",
      "\n",
      "loss for epoch  463  -->  0.21328\n",
      "training_accuracy for epoch  463  -->  0.83670974\n",
      "testing_accuracy for epoch  463  -->  0.82067263\n",
      "\n",
      "\n",
      "loss for epoch  464  -->  0.21328\n",
      "training_accuracy for epoch  464  -->  0.83566034\n",
      "testing_accuracy for epoch  464  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  465  -->  0.21328\n",
      "training_accuracy for epoch  465  -->  0.8368409\n",
      "testing_accuracy for epoch  465  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  466  -->  0.21328\n",
      "training_accuracy for epoch  466  -->  0.8359227\n",
      "testing_accuracy for epoch  466  -->  0.82746214\n",
      "\n",
      "\n",
      "loss for epoch  467  -->  0.21328\n",
      "training_accuracy for epoch  467  -->  0.8359227\n",
      "testing_accuracy for epoch  467  -->  0.8211949\n",
      "\n",
      "\n",
      "loss for epoch  468  -->  0.21328\n",
      "training_accuracy for epoch  468  -->  0.8359227\n",
      "testing_accuracy for epoch  468  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  469  -->  0.21328\n",
      "training_accuracy for epoch  469  -->  0.8350045\n",
      "testing_accuracy for epoch  469  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  470  -->  0.21328\n",
      "training_accuracy for epoch  470  -->  0.8357916\n",
      "testing_accuracy for epoch  470  -->  0.8269399\n",
      "\n",
      "\n",
      "loss for epoch  471  -->  0.21328\n",
      "training_accuracy for epoch  471  -->  0.8348734\n",
      "testing_accuracy for epoch  471  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  472  -->  0.21328\n",
      "training_accuracy for epoch  472  -->  0.83579147\n",
      "testing_accuracy for epoch  472  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  473  -->  0.21328\n",
      "training_accuracy for epoch  473  -->  0.834611\n",
      "testing_accuracy for epoch  473  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  474  -->  0.21328\n",
      "training_accuracy for epoch  474  -->  0.83474225\n",
      "testing_accuracy for epoch  474  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  475  -->  0.21328\n",
      "training_accuracy for epoch  475  -->  0.8368408\n",
      "testing_accuracy for epoch  475  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  476  -->  0.21328\n",
      "training_accuracy for epoch  476  -->  0.8344799\n",
      "testing_accuracy for epoch  476  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  477  -->  0.21328\n",
      "training_accuracy for epoch  477  -->  0.8359227\n",
      "testing_accuracy for epoch  477  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  478  -->  0.21328\n",
      "training_accuracy for epoch  478  -->  0.8343487\n",
      "testing_accuracy for epoch  478  -->  0.82171714\n",
      "\n",
      "\n",
      "loss for epoch  479  -->  0.21328\n",
      "training_accuracy for epoch  479  -->  0.83500445\n",
      "testing_accuracy for epoch  479  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  480  -->  0.21328\n",
      "training_accuracy for epoch  480  -->  0.83526677\n",
      "testing_accuracy for epoch  480  -->  0.8258953\n",
      "\n",
      "\n",
      "loss for epoch  481  -->  0.21328\n",
      "training_accuracy for epoch  481  -->  0.83789015\n",
      "testing_accuracy for epoch  481  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  482  -->  0.21328\n",
      "training_accuracy for epoch  482  -->  0.8365786\n",
      "testing_accuracy for epoch  482  -->  0.8258953\n",
      "\n",
      "\n",
      "loss for epoch  483  -->  0.21328\n",
      "training_accuracy for epoch  483  -->  0.83657855\n",
      "testing_accuracy for epoch  483  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  484  -->  0.21328\n",
      "training_accuracy for epoch  484  -->  0.8351357\n",
      "testing_accuracy for epoch  484  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  485  -->  0.21328\n",
      "training_accuracy for epoch  485  -->  0.8357916\n",
      "testing_accuracy for epoch  485  -->  0.8258953\n",
      "\n",
      "\n",
      "loss for epoch  486  -->  0.21328\n",
      "training_accuracy for epoch  486  -->  0.8350045\n",
      "testing_accuracy for epoch  486  -->  0.82746214\n",
      "\n",
      "\n",
      "loss for epoch  487  -->  0.21328\n",
      "training_accuracy for epoch  487  -->  0.83670974\n",
      "testing_accuracy for epoch  487  -->  0.82641757\n",
      "\n",
      "\n",
      "loss for epoch  488  -->  0.21328\n",
      "training_accuracy for epoch  488  -->  0.8359227\n",
      "testing_accuracy for epoch  488  -->  0.823284\n",
      "\n",
      "\n",
      "loss for epoch  489  -->  0.21328\n",
      "training_accuracy for epoch  489  -->  0.835398\n",
      "testing_accuracy for epoch  489  -->  0.82641757\n",
      "\n",
      "\n",
      "loss for epoch  490  -->  0.21328\n",
      "training_accuracy for epoch  490  -->  0.8348734\n",
      "testing_accuracy for epoch  490  -->  0.82432854\n",
      "\n",
      "\n",
      "loss for epoch  491  -->  0.21328\n",
      "training_accuracy for epoch  491  -->  0.8363163\n",
      "testing_accuracy for epoch  491  -->  0.8227617\n",
      "\n",
      "\n",
      "loss for epoch  492  -->  0.21328\n",
      "training_accuracy for epoch  492  -->  0.8355292\n",
      "testing_accuracy for epoch  492  -->  0.8258953\n",
      "\n",
      "\n",
      "loss for epoch  493  -->  0.21328\n",
      "training_accuracy for epoch  493  -->  0.8364473\n",
      "testing_accuracy for epoch  493  -->  0.82746214\n",
      "\n",
      "\n",
      "loss for epoch  494  -->  0.21328\n",
      "training_accuracy for epoch  494  -->  0.8343487\n",
      "testing_accuracy for epoch  494  -->  0.8248508\n",
      "\n",
      "\n",
      "loss for epoch  495  -->  0.21328\n",
      "training_accuracy for epoch  495  -->  0.83618504\n",
      "testing_accuracy for epoch  495  -->  0.8295512\n",
      "\n",
      "\n",
      "loss for epoch  496  -->  0.21328\n",
      "training_accuracy for epoch  496  -->  0.8357916\n",
      "testing_accuracy for epoch  496  -->  0.82537305\n",
      "\n",
      "\n",
      "loss for epoch  497  -->  0.21328\n",
      "training_accuracy for epoch  497  -->  0.833824\n",
      "testing_accuracy for epoch  497  -->  0.8238062\n",
      "\n",
      "\n",
      "loss for epoch  498  -->  0.21328\n",
      "training_accuracy for epoch  498  -->  0.83566034\n",
      "testing_accuracy for epoch  498  -->  0.82432854\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for epoch  499  -->  0.21328\n",
      "training_accuracy for epoch  499  -->  0.83644736\n",
      "testing_accuracy for epoch  499  -->  0.82171714\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# our main training section\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 25\n",
    "\n",
    "training_acc = [] \n",
    "test_acc = []\n",
    "test_loss = []\n",
    "train_loss  = []\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    epoch_loss = 0 \n",
    "    \n",
    "    no_of_batches = X_train.shape[0]//batch_size\n",
    "    indices = np.arange(no_of_batches)\n",
    "    \n",
    "    X_batch,y_batch = batching(X_train,y_train,batch_size,no_of_batches)\n",
    "    \n",
    "    for i in indices:\n",
    "        _,batch_loss = sess.run([optimizer,mean_loss],feed_dict={X:X_batch[i],targets:y_batch[i]})\n",
    "        epoch_loss = epoch_loss+batch_loss    \n",
    "    \n",
    "    \n",
    "    training_accuracy = sess.run([accuracy],feed_dict={X:X_train,targets:y_train})\n",
    "    testing_accuracy  = sess.run([accuracy],feed_dict={X:X_test,targets:y_test})\n",
    "    mean_epoch_loss = epoch_loss/no_of_batches\n",
    "    testing_loss = sess.run([mean_loss],feed_dict={X:X_test,targets:y_test})\n",
    "\n",
    "    training_acc.append(training_accuracy)\n",
    "    test_acc.append(testing_accuracy)\n",
    "    train_loss.append(mean_epoch_loss)\n",
    "    test_loss.append(testing_loss)\n",
    "    \n",
    "    print('loss for epoch ',e,' --> ',round(mean_epoch_loss,5))\n",
    "    print('training_accuracy for epoch ',e,' --> ',training_accuracy[0])\n",
    "    print('testing_accuracy for epoch ',e,' --> ',testing_accuracy[0])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rms-tf-model/500-epoch-rmsprop-classifier'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'rms-tf-model/500-epoch-rmsprop-classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.83815247]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(training_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8295512]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2d2d5b1b2e8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Accuracy curves')\n",
    "plt.plot(np.arange(epochs),training_acc,label='train accuracy') #training accuracy\n",
    "plt.plot(np.arange(epochs),test_acc,label='test accuracy') #testing accuracy\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2d2d5b7dd68>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('loss curves')\n",
    "plt.plot(np.arange(epochs),train_loss,label='training loss')\n",
    "plt.plot(np.arange(epochs),test_loss,label='testing loss')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
